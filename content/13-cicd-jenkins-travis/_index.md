---
title: "CI/CD Pipeline"
date: 2024-01-01T00:00:00Z
weight: 13
chapter: false
pre: "<b>13. </b>"
---

{{% notice info %}}
**üéØ M·ª•c ti√™u Task 13:**

Thi·∫øt l·∫≠p pipeline CI/CD t·ª± ƒë·ªông cho to√†n b·ªô v√≤ng ƒë·ªùi d·ª± √°n MLOps Retail Prediction:

- CI (Continuous Integration): build & test code, build Docker image, push ECR
- CD (Continuous Delivery): hu·∫•n luy·ªán l·∫°i model, c·∫≠p nh·∫≠t Model Registry, deploy phi√™n b·∫£n m·ªõi l√™n EKS
- Monitoring hook: rollback khi API ho·∫∑c model c√≥ l·ªói (CloudWatch trigger)

‚Üí ƒê·∫£m b·∫£o tri·ªÉn khai li√™n t·ª•c, gi·∫£m l·ªói th·ªß c√¥ng, ti·∫øt ki·ªám th·ªùi gian v√† chi ph√≠.
{{% /notice %}}

## 1. C·∫•u tr√∫c pipeline t·ªïng qu√°t

Pipeline CI/CD c·ªßa d·ª± √°n Retail Prediction s·∫Ω t·ª± ƒë·ªông h√≥a to√†n b·ªô quy tr√¨nh t·ª´ commit code ƒë·∫øn deploy l√™n production, bao g·ªìm c·∫£ vi·ªác hu·∫•n luy·ªán l·∫°i model khi c·∫ßn thi·∫øt.

{{< mermaid >}}
graph TD
    subgraph "Triggers"
        A1[Code Push] --> B
        A2[Data Change] --> B
        A3[Manual Trigger] --> B
    end
    
    B[GitHub Actions Workflow] --> C{Environment?}
    
    C -->|DEV| D1[CI]
    C -->|STAGING| D2[CI + Train]
    C -->|PROD| D3[CI + Train + CD]
    
    subgraph "CI Process"
        D1 --> E1[Test & Validate]
        D2 --> E1
        D3 --> E1
        E1 --> F1[Build Container]
        F1 --> G1[Push to ECR]
    end
    
    subgraph "Training Process"
        D2 --> E2[Prepare Data]
        D3 --> E2
        E2 --> F2[Train Model - SageMaker]
        F2 --> G2[Evaluate Model]
        G2 --> H2{Model Better?}
        H2 -->|Yes| I2[Register in Model Registry]
        H2 -->|No| J2[Keep Previous Model]
    end
    
    subgraph "Deployment Process"
        D3 --> E3[Update K8s Manifests]
        I2 --> E3
        G1 --> E3
        E3 --> F3[Deploy to EKS]
        F3 --> G3[Health Check]
        G3 --> H3{Healthy?}
        H3 -->|Yes| I3[Complete]
        H3 -->|No| J3[Rollback]
    end
    
    subgraph "Monitoring"
        F3 --> M1[CloudWatch Metrics]
        M1 --> M2{Alert Triggered?}
        M2 -->|Yes| J3
    end
    
    classDef trigger fill:#f9f,stroke:#333,stroke-width:1px
    classDef primary fill:#bbf,stroke:#333,stroke-width:2px
    classDef secondary fill:#ddf,stroke:#333,stroke-width:1px
    classDef success fill:#bfb,stroke:#333,stroke-width:1px
    classDef warning fill:#ffb,stroke:#333,stroke-width:1px
    classDef danger fill:#fbb,stroke:#333,stroke-width:1px
    
    class A1,A2,A3 trigger
    class B,C,D3 primary
    class E1,E2,E3,F2,F3 secondary
    class I2,I3 success
    class H2,H3,M2 warning
    class J3 danger
{{< /mermaid >}}

Pipeline chia th√†nh 3 m√¥i tr∆∞·ªùng:
- **DEV**: Build, test v√† validate code
- **STAGING**: Hu·∫•n luy·ªán v√† ƒë√°nh gi√° model
- **PROD**: Deploy v√† monitor tr√™n production

### 2. IAM Roles v√† Permissions

#### 2.1 Create CI/CD Service Role

```bash
# Create trust policy for CI/CD role
cat > cicd-trust-policy.json << EOF
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Principal": {
        "Service": [
          "ec2.amazonaws.com",
          "codebuild.amazonaws.com",
          "codepipeline.amazonaws.com"
        ]
      },
      "Action": "sts:AssumeRole"
    },
    {
      "Effect": "Allow",
      "Principal": {
        "AWS": "arn:aws:iam::YOUR_ACCOUNT_ID:user/jenkins-user"
      },
      "Action": "sts:AssumeRole"
    }
  ]
}
EOF

# Create the role
aws iam create-role \
  --role-name RetailForecastCICDRole \
  --assume-role-policy-document file://cicd-trust-policy.json \
  --description "Role for Retail Forecast CI/CD Pipeline"
```

#### 2.2 CI/CD IAM Policy

```json
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Action": [
        "s3:GetObject",
        "s3:PutObject",
        "s3:DeleteObject",
        "s3:ListBucket"
      ],
      "Resource": [
        "arn:aws:s3:::retail-forecast-data-bucket",
        "arn:aws:s3:::retail-forecast-data-bucket/*",
        "arn:aws:s3:::retail-forecast-artifacts-bucket",
        "arn:aws:s3:::retail-forecast-artifacts-bucket/*"
      ]
    },
    {
      "Effect": "Allow",
      "Action": [
        "ecr:GetAuthorizationToken",
        "ecr:BatchCheckLayerAvailability",
        "ecr:GetDownloadUrlForLayer",
        "ecr:BatchGetImage",
        "ecr:PutImage",
        "ecr:InitiateLayerUpload",
        "ecr:UploadLayerPart",
        "ecr:CompleteLayerUpload"
      ],
      "Resource": "*"
    },
    {
      "Effect": "Allow",
      "Action": [
        "sagemaker:CreateTrainingJob",
        "sagemaker:DescribeTrainingJob",
        "sagemaker:StopTrainingJob",
        "sagemaker:CreateModel",
        "sagemaker:CreateModelPackage",
        "sagemaker:CreateModelPackageGroup",
        "sagemaker:DescribeModelPackage",
        "sagemaker:UpdateModelPackage",
        "sagemaker:ListModelPackages"
      ],
      "Resource": "*"
    },
    {
      "Effect": "Allow",
      "Action": [
        "eks:DescribeCluster",
        "eks:ListClusters",
        "eks:DescribeNodegroup"
      ],
      "Resource": "*"
    },
    {
      "Effect": "Allow",
      "Action": [
        "sts:GetCallerIdentity",
        "sts:AssumeRole"
      ],
      "Resource": "*"
    },
    {
      "Effect": "Allow",
      "Action": [
        "kms:Decrypt",
        "kms:GenerateDataKey",
        "kms:CreateGrant"
      ],
      "Resource": [
        "arn:aws:kms:us-east-1:YOUR_ACCOUNT_ID:key/*"
      ]
    },
    {
      "Effect": "Allow",
      "Action": [
        "logs:CreateLogGroup",
        "logs:CreateLogStream",
        "logs:PutLogEvents"
      ],
      "Resource": "*"
    }
  ]
}
```

#### 2.3 Attach Policies

```bash
# Create and attach the policy
aws iam create-policy \
  --policy-name RetailForecastCICDPolicy \
  --policy-document file://cicd-policy.json

aws iam attach-role-policy \
  --role-name RetailForecastCICDRole \
  --policy-arn arn:aws:iam::YOUR_ACCOUNT_ID:policy/RetailForecastCICDPolicy

# Attach additional managed policies
aws iam attach-role-policy \
  --role-name RetailForecastCICDRole \
  --policy-arn arn:aws:iam::aws:policy/AmazonEKSClusterPolicy

aws iam attach-role-policy \
  --role-name RetailForecastCICDRole \
  --policy-arn arn:aws:iam::aws:policy/AmazonEKSWorkerNodePolicy
```

### 3. Jenkins Pipeline Setup

#### 3.1 Jenkins Installation on EC2

```bash
# Launch EC2 instance for Jenkins
aws ec2 run-instances \
  --image-id ami-0c55b159cbfafe1d0 \
  --instance-type t3.medium \
  --key-name your-key-pair \
  --security-group-ids sg-jenkins \
  --subnet-id subnet-12345678 \
  --iam-instance-profile Name=JenkinsInstanceProfile \
  --user-data file://jenkins-install.sh \
  --tag-specifications 'ResourceType=instance,Tags=[{Key=Name,Value=Jenkins-Server},{Key=Project,Value=RetailForecast}]'
```

#### 3.2 Jenkins Installation Script

```bash
#!/bin/bash
# jenkins-install.sh

# Update system
yum update -y

# Install Docker
yum install -y docker
systemctl start docker
systemctl enable docker
usermod -a -G docker ec2-user

# Install Docker Compose
curl -L "https://github.com/docker/compose/releases/download/1.29.2/docker-compose-$(uname -s)-$(uname -m)" -o /usr/local/bin/docker-compose
chmod +x /usr/local/bin/docker-compose

# Install Jenkins
wget -O /etc/yum.repos.d/jenkins.repo https://pkg.jenkins.io/redhat-stable/jenkins.repo
rpm --import https://pkg.jenkins.io/redhat-stable/jenkins.io.key
yum upgrade -y
yum install -y java-11-openjdk jenkins

# Start Jenkins
systemctl start jenkins
systemctl enable jenkins

# Install kubectl
curl -o kubectl https://amazon-eks.s3.us-west-2.amazonaws.com/1.21.2/2021-07-05/bin/linux/amd64/kubectl
chmod +x ./kubectl
mv ./kubectl /usr/local/bin

# Install AWS CLI v2
curl "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o "awscliv2.zip"
unzip awscliv2.zip
./aws/install

# Install Python and dependencies
yum install -y python3 python3-pip
pip3 install boto3 sagemaker pandas scikit-learn

# Configure Jenkins user for Docker
usermod -a -G docker jenkins
systemctl restart jenkins

echo "Jenkins installation completed!"
echo "Access Jenkins at: http://$(curl -s http://169.254.169.254/latest/meta-data/public-ip):8080"
echo "Initial admin password: $(cat /var/lib/jenkins/secrets/initialAdminPassword)"
```

#### 3.3 Jenkinsfile for ML Pipeline

```groovy
// Jenkinsfile
pipeline {
    agent any
    
    environment {
        AWS_DEFAULT_REGION = 'us-east-1'
        AWS_ACCOUNT_ID = '123456789012'
        ECR_REPOSITORY = 'retail-forecast'
        EKS_CLUSTER_NAME = 'retail-forecast-cluster'
        NAMESPACE = 'mlops'
        S3_DATA_BUCKET = 'retail-forecast-data-bucket'
        S3_ARTIFACTS_BUCKET = 'retail-forecast-artifacts-bucket'
        MODEL_PACKAGE_GROUP = 'retail-forecast-model-group'
    }
    
    parameters {
        choice(
            name: 'DEPLOY_ENVIRONMENT',
            choices: ['dev', 'staging', 'prod'],
            description: 'Target deployment environment'
        )
        booleanParam(
            name: 'RETRAIN_MODEL',
            defaultValue: false,
            description: 'Force model retraining'
        )
        booleanParam(
            name: 'SKIP_TESTS',
            defaultValue: false,
            description: 'Skip test execution'
        )
    }
    
    stages {
        stage('Setup') {
            steps {
                script {
                    // Clean workspace
                    cleanWs()
                    
                    // Checkout code
                    checkout scm
                    
                    // Set build info
                    env.BUILD_VERSION = "${env.BUILD_NUMBER}-${env.GIT_COMMIT.take(7)}"
                    env.IMAGE_TAG = "v${env.BUILD_VERSION}"
                    
                    echo "Build Version: ${env.BUILD_VERSION}"
                    echo "Image Tag: ${env.IMAGE_TAG}"
                }
            }
        }
        
        stage('Environment Setup') {
            steps {
                script {
                    // Install Python dependencies
                    sh '''
                        python3 -m venv venv
                        source venv/bin/activate
                        pip install --upgrade pip
                        pip install -r requirements.txt
                        pip install pytest pytest-cov flake8
                    '''
                    
                    // Configure AWS credentials
                    sh '''
                        aws sts get-caller-identity
                        aws configure set region $AWS_DEFAULT_REGION
                    '''
                    
                    // Configure kubectl
                    sh '''
                        aws eks update-kubeconfig --name $EKS_CLUSTER_NAME --region $AWS_DEFAULT_REGION
                        kubectl cluster-info
                    '''
                }
            }
        }
        
        stage('Code Quality & Testing') {
            when {
                not { params.SKIP_TESTS }
            }
            parallel {
                stage('Linting') {
                    steps {
                        sh '''
                            source venv/bin/activate
                            flake8 src/ --max-line-length=88 --exclude=venv
                        '''
                    }
                }
                
                stage('Unit Tests') {
                    steps {
                        sh '''
                            source venv/bin/activate
                            pytest tests/unit/ -v --cov=src --cov-report=xml --cov-report=html
                        '''
                        
                        // Publish test results
                        publishTestResults testResultsPattern: 'test-results.xml'
                        publishCoverage adapters: [coberturaAdapter('coverage.xml')], sourceFileResolver: sourceFiles('STORE_LAST_BUILD')
                    }
                }
                
                stage('Integration Tests') {
                    steps {
                        sh '''
                            source venv/bin/activate
                            pytest tests/integration/ -v
                        '''
                    }
                }
            }
        }
        
        stage('Data Validation') {
            steps {
                script {
                    sh '''
                        source venv/bin/activate
                        python scripts/validate_data.py \
                            --bucket $S3_DATA_BUCKET \
                            --key training-data/train.csv \
                            --output data-validation-report.json
                    '''
                    
                    // Archive validation report
                    archiveArtifacts artifacts: 'data-validation-report.json', fingerprint: true
                }
            }
        }
        
        stage('Model Training') {
            when {
                anyOf {
                    params.RETRAIN_MODEL
                    changeset "src/training/**"
                    changeset "data/**"
                }
            }
            steps {
                script {
                    sh '''
                        source venv/bin/activate
                        python scripts/trigger_training.py \
                            --job-name "retail-forecast-training-${BUILD_VERSION}" \
                            --data-bucket $S3_DATA_BUCKET \
                            --output-bucket $S3_ARTIFACTS_BUCKET \
                            --role-arn arn:aws:iam::$AWS_ACCOUNT_ID:role/SageMakerExecutionRole
                    '''
                    
                    // Wait for training completion
                    sh '''
                        source venv/bin/activate
                        python scripts/wait_for_training.py \
                            --job-name "retail-forecast-training-${BUILD_VERSION}" \
                            --timeout 3600
                    '''
                }
            }
        }
        
        stage('Model Validation & Registration') {
            when {
                anyOf {
                    params.RETRAIN_MODEL
                    changeset "src/training/**"
                }
            }
            steps {
                script {
                    // Validate model performance
                    sh '''
                        source venv/bin/activate
                        python scripts/validate_model.py \
                            --job-name "retail-forecast-training-${BUILD_VERSION}" \
                            --baseline-accuracy 0.85 \
                            --output model-validation-report.json
                    '''
                    
                    // Register model if validation passes
                    sh '''
                        source venv/bin/activate
                        python scripts/register_model.py \
                            --job-name "retail-forecast-training-${BUILD_VERSION}" \
                            --model-package-group $MODEL_PACKAGE_GROUP \
                            --approval-status "PendingManualApproval" \
                            --model-version $BUILD_VERSION
                    '''
                    
                    archiveArtifacts artifacts: 'model-validation-report.json', fingerprint: true
                }
            }
        }
        
        stage('Docker Build') {
            steps {
                script {
                    // Build Docker image
                    sh '''
                        # Get latest approved model
                        MODEL_URI=$(python scripts/get_latest_model.py --model-package-group $MODEL_PACKAGE_GROUP)
                        echo "Using model: $MODEL_URI"
                        
                        # Build image with model URI
                        docker build \
                            --build-arg MODEL_URI=$MODEL_URI \
                            --build-arg BUILD_VERSION=$BUILD_VERSION \
                            -t $ECR_REPOSITORY:$IMAGE_TAG \
                            -t $ECR_REPOSITORY:latest \
                            .
                    '''
                    
                    // Security scan
                    sh '''
                        docker run --rm -v /var/run/docker.sock:/var/run/docker.sock \
                            -v $(pwd):/root/.cache/ \
                            aquasec/trivy:latest image \
                            --exit-code 1 \
                            --severity HIGH,CRITICAL \
                            $ECR_REPOSITORY:$IMAGE_TAG
                    '''
                }
            }
        }
        
        stage('Push to ECR') {
            steps {
                script {
                    sh '''
                        # Login to ECR
                        aws ecr get-login-password --region $AWS_DEFAULT_REGION | \
                            docker login --username AWS --password-stdin \
                            $AWS_ACCOUNT_ID.dkr.ecr.$AWS_DEFAULT_REGION.amazonaws.com
                        
                        # Tag images
                        docker tag $ECR_REPOSITORY:$IMAGE_TAG \
                            $AWS_ACCOUNT_ID.dkr.ecr.$AWS_DEFAULT_REGION.amazonaws.com/$ECR_REPOSITORY:$IMAGE_TAG
                        
                        docker tag $ECR_REPOSITORY:latest \
                            $AWS_ACCOUNT_ID.dkr.ecr.$AWS_DEFAULT_REGION.amazonaws.com/$ECR_REPOSITORY:latest
                        
                        # Push images
                        docker push $AWS_ACCOUNT_ID.dkr.ecr.$AWS_DEFAULT_REGION.amazonaws.com/$ECR_REPOSITORY:$IMAGE_TAG
                        docker push $AWS_ACCOUNT_ID.dkr.ecr.$AWS_DEFAULT_REGION.amazonaws.com/$ECR_REPOSITORY:latest
                    '''
                }
            }
        }
        
        stage('Deploy to EKS') {
            steps {
                script {
                    // Update Kubernetes deployment
                    sh '''
                        # Update deployment image
                        kubectl set image deployment/retail-forecast-api \
                            retail-forecast-api=$AWS_ACCOUNT_ID.dkr.ecr.$AWS_DEFAULT_REGION.amazonaws.com/$ECR_REPOSITORY:$IMAGE_TAG \
                            -n $NAMESPACE
                        
                        # Wait for rollout
                        kubectl rollout status deployment/retail-forecast-api -n $NAMESPACE --timeout=600s
                        
                        # Verify deployment
                        kubectl get pods -n $NAMESPACE -l app=retail-forecast-api
                    '''
                }
            }
        }
        
        stage('Health Check') {
            steps {
                script {
                    sh '''
                        # Get service endpoint
                        ENDPOINT=$(kubectl get service retail-forecast-service -n $NAMESPACE -o jsonpath='{.status.loadBalancer.ingress[0].hostname}')
                        
                        if [ -z "$ENDPOINT" ]; then
                            ENDPOINT=$(kubectl get service retail-forecast-service -n $NAMESPACE -o jsonpath='{.spec.clusterIP}')
                            kubectl port-forward service/retail-forecast-service 8080:80 -n $NAMESPACE &
                            ENDPOINT="localhost:8080"
                            PORT_FORWARD_PID=$!
                        fi
                        
                        # Wait for service to be ready
                        echo "Waiting for service to be ready..."
                        for i in {1..30}; do
                            if curl -f http://$ENDPOINT/healthz; then
                                echo "Service is healthy!"
                                break
                            fi
                            echo "Attempt $i/30 failed, retrying in 10 seconds..."
                            sleep 10
                        done
                        
                        # Test prediction endpoint
                        curl -X POST http://$ENDPOINT/predict \
                            -H "Content-Type: application/json" \
                            -d '{"features": {"store_id": 1, "product_id": 123, "price": 29.99}}'
                        
                        # Kill port-forward if used
                        if [ ! -z "$PORT_FORWARD_PID" ]; then
                            kill $PORT_FORWARD_PID
                        fi
                    '''
                }
            }
        }
        
        stage('Performance Testing') {
            when {
                environment name: 'DEPLOY_ENVIRONMENT', value: 'prod'
            }
            steps {
                script {
                    sh '''
                        # Run load test
                        python scripts/load_test.py \
                            --endpoint http://$ENDPOINT \
                            --duration 300 \
                            --concurrent-users 10 \
                            --output load-test-report.json
                    '''
                    
                    archiveArtifacts artifacts: 'load-test-report.json', fingerprint: true
                }
            }
        }
    }
    
    post {
        always {
            // Clean up
            sh '''
                docker system prune -f
                rm -rf venv
            '''
            
            // Archive logs
            archiveArtifacts artifacts: 'logs/**', allowEmptyArchive: true
        }
        
        success {
            script {
                // Send success notification
                sh '''
                    aws sns publish \
                        --topic-arn arn:aws:sns:$AWS_DEFAULT_REGION:$AWS_ACCOUNT_ID:deployment-notifications \
                        --message "‚úÖ Deployment successful for build $BUILD_VERSION" \
                        --subject "Retail Forecast Deployment Success"
                '''
            }
        }
        
        failure {
            script {
                // Send failure notification
                sh '''
                    aws sns publish \
                        --topic-arn arn:aws:sns:$AWS_DEFAULT_REGION:$AWS_ACCOUNT_ID:deployment-notifications \
                        --message "‚ùå Deployment failed for build $BUILD_VERSION. Check Jenkins logs." \
                        --subject "Retail Forecast Deployment Failed"
                '''
                
                // Rollback on production failure
                if (params.DEPLOY_ENVIRONMENT == 'prod') {
                    sh '''
                        echo "Rolling back production deployment..."
                        kubectl rollout undo deployment/retail-forecast-api -n $NAMESPACE
                        kubectl rollout status deployment/retail-forecast-api -n $NAMESPACE
                    '''
                }
            }
        }
    }
}
```

## 2. GitHub Actions CI/CD Pipeline

Ch√∫ng ta s·∫Ω s·ª≠ d·ª•ng GitHub Actions ƒë·ªÉ x√¢y d·ª±ng pipeline CI/CD cho d·ª± √°n MLOps Retail Prediction v√¨ kh·∫£ nƒÉng t√≠ch h·ª£p s·∫µn v·ªõi GitHub repository v√† t√≠nh linh ho·∫°t cao.

### 2.1 C·∫•u h√¨nh workflow file

T·∫°o file `.github/workflows/mlops-pipeline.yml`:

```yaml
# .github/workflows/mlops-pipeline.yml
name: MLOps Retail Prediction Pipeline

on:
  push:
    branches: [ main, develop ]
    paths-ignore:
      - '**.md'
      - 'docs/**'
  pull_request:
    branches: [ main ]
  schedule:
    # Ch·∫°y m·ªói tu·∫ßn v√†o th·ª© 2 ƒë·ªÉ ki·ªÉm tra ƒë·ªô ch√≠nh x√°c c·ªßa model
    - cron: '0 2 * * 1'
  workflow_dispatch:
    inputs:
      environment:
        description: 'M√¥i tr∆∞·ªùng tri·ªÉn khai'
        required: true
        default: 'dev'
        type: choice
        options:
        - dev
        - staging
        - prod
      retrain_model:
        description: 'Hu·∫•n luy·ªán l·∫°i model'
        required: false
        default: false
        type: boolean
      deploy_only:
        description: 'Ch·ªâ tri·ªÉn khai, kh√¥ng build/hu·∫•n luy·ªán m·ªõi'
        required: false
        default: false
        type: boolean

env:
  AWS_REGION: us-east-1
  ECR_REPOSITORY: retail-forecast
  EKS_CLUSTER_NAME: retail-forecast-cluster
  S3_DATA_BUCKET: retail-forecast-data
  S3_MODEL_BUCKET: retail-forecast-models
  MODEL_PACKAGE_GROUP: retail-forecast-models

permissions:
  id-token: write   # C·∫ßn thi·∫øt cho OIDC v·ªõi AWS
  contents: read    # C·∫ßn thi·∫øt ƒë·ªÉ checkout code

jobs:
  setup:
    name: Setup Pipeline
    runs-on: ubuntu-latest
    outputs:
      build-id: ${{ steps.generate-id.outputs.build_id }}
      environment: ${{ steps.set-env.outputs.environment }}
      should-train: ${{ steps.set-env.outputs.should_train }}
      should-deploy: ${{ steps.set-env.outputs.should_deploy }}
    
    steps:
    - name: Generate build ID
      id: generate-id
      run: |
        BUILD_ID="build-${GITHUB_RUN_NUMBER}-${GITHUB_SHA::7}"
        echo "build_id=$BUILD_ID" >> $GITHUB_OUTPUT
        echo "Build ID: $BUILD_ID"
    
    - name: Set environment variables
      id: set-env
      run: |
        # X√°c ƒë·ªãnh m√¥i tr∆∞·ªùng
        if [[ "${{ github.event_name }}" == "workflow_dispatch" ]]; then
          ENV="${{ github.event.inputs.environment }}"
        elif [[ "${{ github.ref }}" == "refs/heads/main" ]]; then
          ENV="prod"
        elif [[ "${{ github.ref }}" == "refs/heads/develop" ]]; then
          ENV="staging"
        else
          ENV="dev"
        fi
        echo "environment=$ENV" >> $GITHUB_OUTPUT
        
        # X√°c ƒë·ªãnh c√≥ n√™n hu·∫•n luy·ªán model hay kh√¥ng
        if [[ "${{ github.event.inputs.retrain_model }}" == "true" ]] || \
           [[ "${{ github.event_name }}" == "schedule" ]]; then
          SHOULD_TRAIN="true"
        else
          SHOULD_TRAIN="false"
        fi
        echo "should_train=$SHOULD_TRAIN" >> $GITHUB_OUTPUT
        
        # X√°c ƒë·ªãnh c√≥ n√™n deploy hay kh√¥ng
        if [[ "${{ github.event.inputs.deploy_only }}" == "true" ]] || \
           [[ "$ENV" == "prod" && "${{ github.ref }}" == "refs/heads/main" ]] || \
           [[ "$ENV" == "staging" && "${{ github.ref }}" == "refs/heads/develop" ]]; then
          SHOULD_DEPLOY="true"
        else
          SHOULD_DEPLOY="false"
        fi
        echo "should_deploy=$SHOULD_DEPLOY" >> $GITHUB_OUTPUT
        
        # In th√¥ng tin
        echo "Environment: $ENV"
        echo "Should train model: $SHOULD_TRAIN"
        echo "Should deploy: $SHOULD_DEPLOY"

  test:
    name: Code Quality & Testing
    needs: setup
    runs-on: ubuntu-latest
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
    
    - name: Setup Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.10'
        cache: 'pip'
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r core/requirements.txt
        pip install -r server/requirements.txt
        pip install pytest pytest-cov pylint black
    
    - name: Code formatting check
      run: |
        black --check core/ server/
    
    - name: Lint code
      run: |
        pylint --disable=C0111,C0103 core/ server/
    
    - name: Run tests
      run: |
        pytest tests/ -v --cov=core --cov=server --cov-report=xml
    
    - name: Upload coverage report
      uses: codecov/codecov-action@v3
      with:
        file: ./coverage.xml
        fail_ci_if_error: false

  data_validation:
    name: Data Validation
    needs: [setup, test]
    runs-on: ubuntu-latest
    if: needs.setup.outputs.should_train == 'true'
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
    
    - name: Configure AWS credentials
      uses: aws-actions/configure-aws-credentials@v2
      with:
        role-to-assume: arn:aws:iam::${{ secrets.AWS_ACCOUNT_ID }}:role/GitHubActionsRole
        aws-region: ${{ env.AWS_REGION }}
    
    - name: Setup Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.10'
        cache: 'pip'
    
    - name: Install dependencies
      run: |
        pip install pandas boto3 great_expectations
    
    - name: Validate training data
      run: |
        python aws/script/validate_data.py \
          --bucket ${{ env.S3_DATA_BUCKET }} \
          --key training/sales_data.csv \
          --output validation_report.json
    
    - name: Upload validation report
      uses: actions/upload-artifact@v3
      with:
        name: data-validation-report
        path: validation_report.json

  model_training:
    name: Model Training
    needs: [setup, test, data_validation]
    runs-on: ubuntu-latest
    if: needs.setup.outputs.should_train == 'true' && success()
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
    
    - name: Configure AWS credentials
      uses: aws-actions/configure-aws-credentials@v2
      with:
        role-to-assume: arn:aws:iam::${{ secrets.AWS_ACCOUNT_ID }}:role/GitHubActionsRole
        aws-region: ${{ env.AWS_REGION }}
    
    - name: Setup Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.10'
        cache: 'pip'
    
    - name: Install dependencies
      run: |
        pip install boto3 sagemaker pandas scikit-learn
    
    - name: Create training job
      id: training
      run: |
        python aws/script/create_training_job.py \
          --job-name "retail-forecast-${{ needs.setup.outputs.build-id }}" \
          --data-bucket ${{ env.S3_DATA_BUCKET }} \
          --output-bucket ${{ env.S3_MODEL_BUCKET }} \
          --instance-type ml.m5.large \
          --hyperparameters "{\"n_estimators\":\"200\",\"max_depth\":\"10\"}"
      
    - name: Wait for training completion
      run: |
        aws sagemaker wait training-job-completed-or-stopped \
          --training-job-name "retail-forecast-${{ needs.setup.outputs.build-id }}"
        
        STATUS=$(aws sagemaker describe-training-job \
          --training-job-name "retail-forecast-${{ needs.setup.outputs.build-id }}" \
          --query 'TrainingJobStatus' --output text)
          
        if [ "$STATUS" != "Completed" ]; then
          echo "Training failed with status: $STATUS"
          exit 1
        fi

  model_evaluation:
    name: Model Evaluation & Registration
    needs: [setup, model_training]
    runs-on: ubuntu-latest
    outputs:
      model_approved: ${{ steps.evaluate.outputs.model_approved }}
      model_version: ${{ steps.register.outputs.model_version }}
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
    
    - name: Configure AWS credentials
      uses: aws-actions/configure-aws-credentials@v2
      with:
        role-to-assume: arn:aws:iam::${{ secrets.AWS_ACCOUNT_ID }}:role/GitHubActionsRole
        aws-region: ${{ env.AWS_REGION }}
    
    - name: Setup Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.10'
        cache: 'pip'
    
    - name: Install dependencies
      run: |
        pip install boto3 sagemaker pandas scikit-learn matplotlib seaborn
    
    - name: Evaluate model
      id: evaluate
      run: |
        python aws/script/processing_evaluate.py \
          --job-name "retail-forecast-${{ needs.setup.outputs.build-id }}" \
          --evaluation-data s3://${{ env.S3_DATA_BUCKET }}/validation/sales_data.csv \
          --baseline-metrics s3://${{ env.S3_MODEL_BUCKET }}/baselines/metrics.json \
          --threshold 0.85
          
        # Check result
        if [ -f "model_approved.txt" ]; then
          MODEL_APPROVED=$(cat model_approved.txt)
          echo "model_approved=$MODEL_APPROVED" >> $GITHUB_OUTPUT
          echo "Model evaluation result: $MODEL_APPROVED"
        else
          echo "model_approved=false" >> $GITHUB_OUTPUT
          echo "Model evaluation failed"
          exit 1
        fi
    
    - name: Register model
      id: register
      if: steps.evaluate.outputs.model_approved == 'true'
      run: |
        MODEL_VERSION=$(python aws/script/register_model.py \
          --job-name "retail-forecast-${{ needs.setup.outputs.build-id }}" \
          --model-package-group ${{ env.MODEL_PACKAGE_GROUP }} \
          --approval-status "Approved")
          
        echo "model_version=$MODEL_VERSION" >> $GITHUB_OUTPUT
        echo "Model registered with version: $MODEL_VERSION"
    
    - name: Upload evaluation results
      uses: actions/upload-artifact@v3
      with:
        name: model-evaluation-report
        path: |
          evaluation_results.json
          plots/*.png

  build_docker:
    name: Build & Push Docker Image
    needs: [setup, test]
    runs-on: ubuntu-latest
    outputs:
      image_tag: ${{ steps.build.outputs.image_tag }}
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
    
    - name: Configure AWS credentials
      uses: aws-actions/configure-aws-credentials@v2
      with:
        role-to-assume: arn:aws:iam::${{ secrets.AWS_ACCOUNT_ID }}:role/GitHubActionsRole
        aws-region: ${{ env.AWS_REGION }}
    
    - name: Login to Amazon ECR
      id: login-ecr
      uses: aws-actions/amazon-ecr-login@v1
    
    - name: Build and push Docker image
      id: build
      env:
        ECR_REGISTRY: ${{ steps.login-ecr.outputs.registry }}
      run: |
        # Set tag
        IMAGE_TAG="${{ needs.setup.outputs.build-id }}"
        echo "image_tag=$IMAGE_TAG" >> $GITHUB_OUTPUT
        
        # Get latest model URI (or use placeholder for dev environment)
        if [[ "${{ needs.setup.outputs.environment }}" == "dev" ]]; then
          MODEL_URI="placeholder"
        else
          MODEL_URI=$(aws sagemaker list-model-packages \
            --model-package-group-name ${{ env.MODEL_PACKAGE_GROUP }} \
            --sort-by CreationTime --sort-order Descending --max-items 1 \
            --query 'ModelPackageSummaries[0].ModelPackageArn' --output text)
        fi
        
        echo "Using model URI: $MODEL_URI"
        
        # Build image
        docker build -t $ECR_REGISTRY/$ECR_REPOSITORY:$IMAGE_TAG \
          --build-arg MODEL_URI=$MODEL_URI \
          --build-arg BUILD_ID=$IMAGE_TAG \
          --build-arg ENV=${{ needs.setup.outputs.environment }} \
          server/
        
        # Also tag as latest-{environment}
        docker tag $ECR_REGISTRY/$ECR_REPOSITORY:$IMAGE_TAG \
          $ECR_REGISTRY/$ECR_REPOSITORY:latest-${{ needs.setup.outputs.environment }}
        
        # Security scan
        docker run --rm -v /var/run/docker.sock:/var/run/docker.sock \
          aquasec/trivy:latest image --severity HIGH,CRITICAL \
          $ECR_REGISTRY/$ECR_REPOSITORY:$IMAGE_TAG
        
        # Push images
        docker push $ECR_REGISTRY/$ECR_REPOSITORY:$IMAGE_TAG
        docker push $ECR_REGISTRY/$ECR_REPOSITORY:latest-${{ needs.setup.outputs.environment }}
        
        echo "Image built and pushed: $ECR_REGISTRY/$ECR_REPOSITORY:$IMAGE_TAG"

  deploy_eks:
    name: Deploy to EKS
    needs: [setup, test, build_docker, model_evaluation]
    runs-on: ubuntu-latest
    if: needs.setup.outputs.should_deploy == 'true'
    environment:
      name: ${{ needs.setup.outputs.environment }}
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
    
    - name: Configure AWS credentials
      uses: aws-actions/configure-aws-credentials@v2
      with:
        role-to-assume: arn:aws:iam::${{ secrets.AWS_ACCOUNT_ID }}:role/GitHubActionsRole
        aws-region: ${{ env.AWS_REGION }}
    
    - name: Login to Amazon ECR
      id: login-ecr
      uses: aws-actions/amazon-ecr-login@v1
    
    - name: Update kubeconfig
      run: |
        aws eks update-kubeconfig --name ${{ env.EKS_CLUSTER_NAME }} --region ${{ env.AWS_REGION }}
    
    - name: Check if need to deploy new model
      id: check-model
      run: |
        if [[ "${{ needs.model_evaluation.outputs.model_approved }}" == "true" ]]; then
          echo "deploy_new_model=true" >> $GITHUB_OUTPUT
        else
          echo "deploy_new_model=false" >> $GITHUB_OUTPUT
        fi
    
    - name: Deploy to EKS
      run: |
        # Set environment variables
        NAMESPACE="retail-forecast-${{ needs.setup.outputs.environment }}"
        IMAGE_TAG="${{ needs.build_docker.outputs.image_tag }}"
        ECR_REGISTRY="${{ steps.login-ecr.outputs.registry }}"
        
        # Update kustomization file with new image
        cd aws/k8s
        kustomize edit set image retail-forecast-api=$ECR_REGISTRY/$ECR_REPOSITORY:$IMAGE_TAG
        
        # Apply changes
        kubectl apply -k overlays/${{ needs.setup.outputs.environment }}/ --namespace $NAMESPACE
        
        # Wait for deployment to complete
        kubectl rollout status deployment/retail-forecast-api -n $NAMESPACE --timeout=300s
    
    - name: Create SageMaker endpoint (if new model approved)
      if: steps.check-model.outputs.deploy_new_model == 'true'
      run: |
        python aws/script/deploy_endpoint.py \
          --model-package-arn "arn:aws:sagemaker:${{ env.AWS_REGION }}:${{ secrets.AWS_ACCOUNT_ID }}:model-package/${{ env.MODEL_PACKAGE_GROUP }}/${{ needs.model_evaluation.outputs.model_version }}" \
          --endpoint-name "retail-forecast-${{ needs.setup.outputs.environment }}" \
          --instance-type ml.t2.medium \
          --instance-count 1
    
    - name: Health check
      run: |
        # Get service endpoint
        NAMESPACE="retail-forecast-${{ needs.setup.outputs.environment }}"
        SERVICE_HOST=$(kubectl get ingress -n $NAMESPACE retail-forecast-ingress -o jsonpath='{.status.loadBalancer.ingress[0].hostname}')
        
        # If running locally, use port-forwarding
        if [ -z "$SERVICE_HOST" ]; then
          echo "No external hostname found, using port-forwarding"
          kubectl port-forward svc/retail-forecast-service 8080:80 -n $NAMESPACE &
          sleep 5
          SERVICE_HOST="localhost:8080"
        fi
        
        # Check health endpoint
        echo "Testing health endpoint: http://$SERVICE_HOST/health"
        HEALTH_STATUS=$(curl -s -o /dev/null -w "%{http_code}" http://$SERVICE_HOST/health)
        
        if [ "$HEALTH_STATUS" -eq 200 ]; then
          echo "Health check passed: $HEALTH_STATUS"
        else
          echo "Health check failed: $HEALTH_STATUS"
          exit 1
        fi
        
        # Test prediction endpoint
        echo "Testing prediction endpoint"
        PREDICTION_RESULT=$(curl -s -X POST \
          -H "Content-Type: application/json" \
          -d '{"store_id": 1, "item_id": 123, "date": "2023-06-15"}' \
          http://$SERVICE_HOST/predict)
        
        echo "Prediction result: $PREDICTION_RESULT"
        
        # Check if response contains prediction
        if [[ $PREDICTION_RESULT == *"prediction"* ]]; then
          echo "Prediction endpoint working correctly"
        else
          echo "Prediction endpoint not returning expected response"
          exit 1
        fi

  monitoring:
    name: Setup Monitoring
    needs: [setup, deploy_eks]
    runs-on: ubuntu-latest
    if: always() && needs.deploy_eks.result == 'success'
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
    
    - name: Configure AWS credentials
      uses: aws-actions/configure-aws-credentials@v2
      with:
        role-to-assume: arn:aws:iam::${{ secrets.AWS_ACCOUNT_ID }}:role/GitHubActionsRole
        aws-region: ${{ env.AWS_REGION }}
    
    - name: Setup CloudWatch alarms for new deployment
      run: |
        # Create/update CloudWatch alarms for API and model performance
        NAMESPACE="retail-forecast-${{ needs.setup.outputs.environment }}"
        DEPLOYMENT_ID="${{ needs.setup.outputs.build-id }}"
        
        aws cloudwatch put-metric-alarm \
          --alarm-name "RetailForecast-API-Error-Rate-$NAMESPACE" \
          --alarm-description "Monitor error rate for Retail Forecast API" \
          --metric-name "ErrorRate" \
          --namespace "RetailForecast/$NAMESPACE" \
          --statistic "Average" \
          --period 60 \
          --threshold 5 \
          --comparison-operator "GreaterThanThreshold" \
          --evaluation-periods 3 \
          --alarm-actions "arn:aws:sns:${{ env.AWS_REGION }}:${{ secrets.AWS_ACCOUNT_ID }}:retail-forecast-alerts" \
          --dimensions "Name=DeploymentId,Value=$DEPLOYMENT_ID"
        
        aws cloudwatch put-metric-alarm \
          --alarm-name "RetailForecast-API-Latency-$NAMESPACE" \
          --alarm-description "Monitor latency for Retail Forecast API" \
          --metric-name "Latency" \
          --namespace "RetailForecast/$NAMESPACE" \
          --statistic "Average" \
          --period 60 \
          --threshold 1000 \
          --comparison-operator "GreaterThanThreshold" \
          --evaluation-periods 3 \
          --alarm-actions "arn:aws:sns:${{ env.AWS_REGION }}:${{ secrets.AWS_ACCOUNT_ID }}:retail-forecast-alerts" \
          --dimensions "Name=DeploymentId,Value=$DEPLOYMENT_ID"
        
        echo "CloudWatch alarms configured successfully"

  notify:
    name: Send Notifications
    needs: [setup, deploy_eks, monitoring]
    runs-on: ubuntu-latest
    if: always()
    
    steps:
    - name: Determine workflow status
      id: status
      run: |
        if [[ "${{ needs.deploy_eks.result }}" == "success" ]]; then
          echo "result=success" >> $GITHUB_OUTPUT
          echo "message=‚úÖ Deployment successful for ${{ needs.setup.outputs.environment }} environment (Build ${{ needs.setup.outputs.build-id }})" >> $GITHUB_OUTPUT
        elif [[ "${{ needs.deploy_eks.result }}" == "failure" ]]; then
          echo "result=failure" >> $GITHUB_OUTPUT
          echo "message=‚ùå Deployment failed for ${{ needs.setup.outputs.environment }} environment (Build ${{ needs.setup.outputs.build-id }})" >> $GITHUB_OUTPUT
        else
          echo "result=skipped" >> $GITHUB_OUTPUT
          echo "message=‚ÑπÔ∏è Deployment skipped for ${{ needs.setup.outputs.environment }} environment (Build ${{ needs.setup.outputs.build-id }})" >> $GITHUB_OUTPUT
        fi
    
    - name: Configure AWS credentials
      if: steps.status.outputs.result != 'skipped'
      uses: aws-actions/configure-aws-credentials@v2
      with:
        role-to-assume: arn:aws:iam::${{ secrets.AWS_ACCOUNT_ID }}:role/GitHubActionsRole
        aws-region: ${{ env.AWS_REGION }}
    
    - name: Send SNS notification
      if: steps.status.outputs.result != 'skipped'
      run: |
        aws sns publish \
          --topic-arn "arn:aws:sns:${{ env.AWS_REGION }}:${{ secrets.AWS_ACCOUNT_ID }}:retail-forecast-alerts" \
          --subject "Retail Forecast Deployment: ${{ steps.status.outputs.result }}" \
          --message "${{ steps.status.outputs.message }}"
```

## 3. IAM Role v√† Quy·ªÅn h·∫°n

ƒê·ªÉ GitHub Actions c√≥ th·ªÉ t∆∞∆°ng t√°c v·ªõi c√°c d·ªãch v·ª• AWS, ch√∫ng ta c·∫ßn thi·∫øt l·∫≠p IAM Role v·ªõi quy·ªÅn h·∫°n ph√π h·ª£p v√† s·ª≠ d·ª•ng OIDC ƒë·ªÉ x√°c th·ª±c.

### 3.1 T·∫°o IAM Role cho GitHub Actions

```bash
# T·∫°o IAM Role cho GitHub Actions
cat > trust-policy.json << EOF
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Principal": {
        "Federated": "arn:aws:iam::<ACCOUNT_ID>:oidc-provider/token.actions.githubusercontent.com"
      },
      "Action": "sts:AssumeRoleWithWebIdentity",
      "Condition": {
        "StringEquals": {
          "token.actions.githubusercontent.com:aud": "sts.amazonaws.com"
        },
        "StringLike": {
          "token.actions.githubusercontent.com:sub": "repo:<GITHUB_ORG>/<REPO_NAME>:*"
        }
      }
    }
  ]
}
EOF

# T·∫°o role
aws iam create-role --role-name GitHubActionsRole \
  --assume-role-policy-document file://trust-policy.json \
  --description "Role for GitHub Actions CI/CD pipeline"
```

### 3.2 IAM Policy cho CI/CD Pipeline

```json
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Action": [
        "s3:GetObject",
        "s3:PutObject",
        "s3:ListBucket"
      ],
      "Resource": [
        "arn:aws:s3:::retail-forecast-data*",
        "arn:aws:s3:::retail-forecast-data*/*",
        "arn:aws:s3:::retail-forecast-models*",
        "arn:aws:s3:::retail-forecast-models*/*"
      ]
    },
    {
      "Effect": "Allow",
      "Action": [
        "sagemaker:CreateTrainingJob",
        "sagemaker:DescribeTrainingJob",
        "sagemaker:StopTrainingJob",
        "sagemaker:ListTrainingJobs",
        "sagemaker:CreateProcessingJob",
        "sagemaker:DescribeProcessingJob",
        "sagemaker:StopProcessingJob",
        "sagemaker:CreateModel",
        "sagemaker:DeleteModel",
        "sagemaker:DescribeModel",
        "sagemaker:CreateEndpoint",
        "sagemaker:DescribeEndpoint",
        "sagemaker:DeleteEndpoint",
        "sagemaker:UpdateEndpoint",
        "sagemaker:CreateEndpointConfig",
        "sagemaker:DeleteEndpointConfig",
        "sagemaker:DescribeEndpointConfig",
        "sagemaker:CreateModelPackageGroup",
        "sagemaker:DescribeModelPackageGroup",
        "sagemaker:ListModelPackageGroups",
        "sagemaker:CreateModelPackage",
        "sagemaker:DescribeModelPackage",
        "sagemaker:ListModelPackages",
        "sagemaker:UpdateModelPackage"
      ],
      "Resource": "*"
    },
    {
      "Effect": "Allow",
      "Action": [
        "ecr:GetAuthorizationToken",
        "ecr:BatchCheckLayerAvailability",
        "ecr:GetDownloadUrlForLayer",
        "ecr:BatchGetImage",
        "ecr:InitiateLayerUpload",
        "ecr:UploadLayerPart",
        "ecr:CompleteLayerUpload",
        "ecr:PutImage",
        "ecr:ListImages",
        "ecr:DescribeImages"
      ],
      "Resource": "*"
    },
    {
      "Effect": "Allow",
      "Action": [
        "eks:DescribeCluster",
        "eks:ListClusters"
      ],
      "Resource": "*"
    },
    {
      "Effect": "Allow",
      "Action": [
        "cloudwatch:PutMetricData",
        "cloudwatch:GetMetricData",
        "cloudwatch:PutMetricAlarm",
        "cloudwatch:DescribeAlarms",
        "cloudwatch:DeleteAlarms"
      ],
      "Resource": "*"
    },
    {
      "Effect": "Allow",
      "Action": [
        "logs:CreateLogGroup",
        "logs:CreateLogStream",
        "logs:PutLogEvents",
        "logs:DescribeLogStreams"
      ],
      "Resource": "*"
    },
    {
      "Effect": "Allow",
      "Action": [
        "sns:Publish"
      ],
      "Resource": "arn:aws:sns:*:*:retail-forecast-*"
    }
  ]
}
```

### 3.3 G·∫Øn Policy cho Role

```bash
# T·∫°o v√† g·∫Øn policy
aws iam create-policy \
  --policy-name GitHubActionsPolicy \
  --policy-document file://github-actions-policy.json

aws iam attach-role-policy \
  --role-name GitHubActionsRole \
  --policy-arn arn:aws:iam::<ACCOUNT_ID>:policy/GitHubActionsPolicy

# G·∫Øn th√™m c√°c managed policy c·∫ßn thi·∫øt
aws iam attach-role-policy \
  --role-name GitHubActionsRole \
  --policy-arn arn:aws:iam::aws:policy/AmazonEKSClusterPolicy

aws iam attach-role-policy \
  --role-name GitHubActionsRole \
  --policy-arn arn:aws:iam::aws:policy/AmazonECR-FullAccess
```

## 4. Script h·ªó tr·ª£ cho Pipeline

### 4.1 Script t·∫°o Training Job

```python
# aws/script/create_training_job.py
import boto3
import argparse
import json
import os
from datetime import datetime

def create_training_job(job_name, data_bucket, output_bucket, 
                        instance_type='ml.m5.large', 
                        hyperparameters=None):
    """
    T·∫°o SageMaker training job cho Retail Forecast model
    """
    sagemaker = boto3.client('sagemaker')
    
    if hyperparameters is None:
        hyperparameters = {
            'n_estimators': '100',
            'max_depth': '10',
            'random_state': '42'
        }
    elif isinstance(hyperparameters, str):
        hyperparameters = json.loads(hyperparameters)
    
    # L·∫•y SageMaker execution role t·ª´ m√¥i tr∆∞·ªùng ho·∫∑c t·∫°o m·ªõi
    role_arn = os.environ.get('SAGEMAKER_ROLE_ARN')
    if not role_arn:
        # T√¨m role m·∫∑c ƒë·ªãnh
        iam = boto3.client('iam')
        paginator = iam.get_paginator('list_roles')
        for page in paginator.paginate():
            for role in page['Roles']:
                if 'AmazonSageMaker-ExecutionRole' in role['RoleName']:
                    role_arn = role['Arn']
                    break
    
    if not role_arn:
        raise ValueError("Kh√¥ng t√¨m th·∫•y SageMaker execution role")
    
    # C·∫•u h√¨nh training job
    training_params = {
        'TrainingJobName': job_name,
        'HyperParameters': hyperparameters,
        'AlgorithmSpecification': {
            'TrainingImage': '683313688378.dkr.ecr.us-east-1.amazonaws.com/sagemaker-scikit-learn:1.0-1-cpu-py3',
            'TrainingInputMode': 'File'
        },
        'RoleArn': role_arn,
        'InputDataConfig': [
            {
                'ChannelName': 'train',
                'DataSource': {
                    'S3DataSource': {
                        'S3DataType': 'S3Prefix',
                        'S3Uri': f's3://{data_bucket}/training/',
                        'S3DataDistributionType': 'FullyReplicated'
                    }
                },
                'ContentType': 'text/csv'
            },
            {
                'ChannelName': 'validation',
                'DataSource': {
                    'S3DataSource': {
                        'S3DataType': 'S3Prefix',
                        'S3Uri': f's3://{data_bucket}/validation/',
                        'S3DataDistributionType': 'FullyReplicated'
                    }
                },
                'ContentType': 'text/csv'
            }
        ],
        'OutputDataConfig': {
            'S3OutputPath': f's3://{output_bucket}/models/'
        },
        'ResourceConfig': {
            'InstanceType': instance_type,
            'InstanceCount': 1,
            'VolumeSizeInGB': 30
        },
        'StoppingCondition': {
            'MaxRuntimeInSeconds': 3600
        },
        'Tags': [
            {'Key': 'Project', 'Value': 'RetailForecast'},
            {'Key': 'CreatedBy', 'Value': 'GitHubActions'},
            {'Key': 'Environment', 'Value': os.environ.get('ENVIRONMENT', 'dev')}
        ]
    }
    
    # T·∫°o training job
    response = sagemaker.create_training_job(**training_params)
    print(f"Training job created: {job_name}")
    print(f"ARN: {response['TrainingJobArn']}")
    
    return response['TrainingJobArn']

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='Create SageMaker training job')
    parser.add_argument('--job-name', type=str, required=True, 
                        help='Name for the training job')
    parser.add_argument('--data-bucket', type=str, required=True,
                        help='S3 bucket containing training data')
    parser.add_argument('--output-bucket', type=str, required=True,
                        help='S3 bucket for output artifacts')
    parser.add_argument('--instance-type', type=str, default='ml.m5.large',
                        help='SageMaker instance type')
    parser.add_argument('--hyperparameters', type=str, default=None,
                        help='JSON string of hyperparameters')
    
    args = parser.parse_args()
    
    create_training_job(
        args.job_name,
        args.data_bucket,
        args.output_bucket,
        args.instance_type,
        args.hyperparameters
    )
```

### 4.2 Script ƒë√°nh gi√° Model

```python
# aws/script/processing_evaluate.py
import boto3
import argparse
import json
import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

def evaluate_model(job_name, evaluation_data, baseline_metrics=None, threshold=0.85):
    """ƒê√°nh gi√° model v√† so s√°nh v·ªõi baseline metrics"""
    
    # T·∫°o th∆∞ m·ª•c cho plots
    os.makedirs('plots', exist_ok=True)
    
    sagemaker = boto3.client('sagemaker')
    s3 = boto3.client('s3')
    
    # L·∫•y th√¥ng tin training job
    response = sagemaker.describe_training_job(TrainingJobName=job_name)
    if response['TrainingJobStatus'] != 'Completed':
        raise Exception(f"Training job {job_name} ch∆∞a ho√†n th√†nh")
    
    model_artifacts = response['ModelArtifacts']['S3ModelArtifacts']
    print(f"Model artifacts: {model_artifacts}")
    
    # Download d·ªØ li·ªáu ƒë√°nh gi√° t·ª´ S3
    if evaluation_data.startswith('s3://'):
        bucket, key = evaluation_data.replace('s3://', '').split('/', 1)
        local_path = 'evaluation_data.csv'
        s3.download_file(bucket, key, local_path)
    else:
        local_path = evaluation_data
    
    # Load d·ªØ li·ªáu ƒë√°nh gi√°
    eval_data = pd.read_csv(local_path)
    
    # Trong d·ª± √°n th·ª±c t·∫ø, ·ªü ƒë√¢y s·∫Ω load model t·ª´ S3 v√† ƒë√°nh gi√°
    # V√≠ d·ª• ƒë∆°n gi·∫£n n√†y gi·∫£ ƒë·ªãnh ch√∫ng ta ƒë√£ c√≥ k·∫øt qu·∫£ d·ª± ƒëo√°n
    
    # Gi·∫£ ƒë·ªãnh k·∫øt qu·∫£ ƒë√°nh gi√°
    # Trong th·ª±c t·∫ø, b·∫°n s·∫Ω load model v√† th·ª±c hi·ªán d·ª± ƒëo√°n
    y_true = eval_data['target'].values
    y_pred = y_true * 0.9 + np.random.normal(0, 0.2, size=y_true.shape)
    
    # T√≠nh to√°n metrics
    metrics = {
        'mse': mean_squared_error(y_true, y_pred),
        'rmse': np.sqrt(mean_squared_error(y_true, y_pred)),
        'mae': mean_absolute_error(y_true, y_pred),
        'r2_score': r2_score(y_true, y_pred),
        'accuracy': 0.88  # Gi·∫£ ƒë·ªãnh cho forecasting task
    }
    
    # So s√°nh v·ªõi baseline metrics n·∫øu c√≥
    baseline = {}
    if baseline_metrics:
        if baseline_metrics.startswith('s3://'):
            bucket, key = baseline_metrics.replace('s3://', '').split('/', 1)
            local_baseline = 'baseline_metrics.json'
            try:
                s3.download_file(bucket, key, local_baseline)
                with open(local_baseline, 'r') as f:
                    baseline = json.load(f)
            except:
                print(f"Kh√¥ng t√¨m th·∫•y baseline metrics: {baseline_metrics}")
        else:
            try:
                with open(baseline_metrics, 'r') as f:
                    baseline = json.load(f)
            except:
                print(f"Kh√¥ng t√¨m th·∫•y baseline metrics: {baseline_metrics}")
    
    # Quy·∫øt ƒë·ªãnh model c√≥ ƒë∆∞·ª£c ch·∫•p nh·∫≠n hay kh√¥ng
    approved = True
    if baseline:
        print("So s√°nh v·ªõi baseline:")
        for key in metrics:
            if key in baseline:
                improvement = (metrics[key] - baseline[key]) / baseline[key] * 100
                print(f"{key}: {metrics[key]:.4f} (baseline: {baseline[key]:.4f}, {improvement:+.2f}%)")
                
                # Ki·ªÉm tra ti√™u ch√≠ c·∫£i thi·ªán
                if key == 'accuracy' and metrics[key] < threshold:
                    approved = False
                elif key in ['mse', 'rmse', 'mae'] and metrics[key] > baseline[key] * 1.1:  # Cho ph√©p t·ªá h∆°n 10%
                    approved = False
    else:
        print("Kh√¥ng c√≥ baseline ƒë·ªÉ so s√°nh")
    
    # T·∫°o visualizations
    plt.figure(figsize=(10, 6))
    plt.scatter(y_true, y_pred, alpha=0.5)
    plt.plot([y_true.min(), y_true.max()], [y_true.min(), y_true.max()], 'r--')
    plt.xlabel('Actual')
    plt.ylabel('Predicted')
    plt.title('Actual vs Predicted Values')
    plt.savefig('plots/actual_vs_predicted.png')
    
    # Residual plot
    residuals = y_true - y_pred
    plt.figure(figsize=(10, 6))
    plt.scatter(y_pred, residuals, alpha=0.5)
    plt.hlines(y=0, xmin=y_pred.min(), xmax=y_pred.max(), colors='r', linestyles='--')
    plt.xlabel('Predicted')
    plt.ylabel('Residuals')
    plt.title('Residual Plot')
    plt.savefig('plots/residuals.png')
    
    # L∆∞u k·∫øt qu·∫£ ƒë√°nh gi√°
    result = {
        'job_name': job_name,
        'metrics': metrics,
        'baseline': baseline,
        'approved': approved,
        'model_uri': model_artifacts
    }
    
    with open('evaluation_results.json', 'w') as f:
        json.dump(result, f, indent=2)
    
    # L∆∞u k·∫øt qu·∫£ approved ƒë·ªÉ GitHub Actions c√≥ th·ªÉ ƒë·ªçc
    with open('model_approved.txt', 'w') as f:
        f.write(str(approved).lower())
    
    print(f"Model approved: {approved}")
    return approved

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='Evaluate trained model')
    parser.add_argument('--job-name', type=str, required=True,
                        help='Name of the SageMaker training job')
    parser.add_argument('--evaluation-data', type=str, required=True,
                        help='S3 URI or local path to evaluation data')
    parser.add_argument('--baseline-metrics', type=str, default=None,
                        help='S3 URI or local path to baseline metrics')
    parser.add_argument('--threshold', type=float, default=0.85,
                        help='Accuracy threshold for model approval')
    
    args = parser.parse_args()
    
    evaluate_model(
        args.job_name,
        args.evaluation_data,
        args.baseline_metrics,
        args.threshold
    )
```

### 4.3 Script ƒëƒÉng k√Ω Model Registry

```python
# aws/script/register_model.py
import boto3
import argparse
import json
import time

def register_model(job_name, model_package_group, approval_status='PendingManualApproval'):
    """ƒêƒÉng k√Ω model v√†o Model Registry"""
    
    sagemaker = boto3.client('sagemaker')
    
    # Ki·ªÉm tra xem Model Package Group ƒë√£ t·ªìn t·∫°i ch∆∞a
    try:
        sagemaker.describe_model_package_group(ModelPackageGroupName=model_package_group)
        print(f"Model Package Group {model_package_group} ƒë√£ t·ªìn t·∫°i")
    except sagemaker.exceptions.ResourceNotFound:
        print(f"T·∫°o Model Package Group m·ªõi: {model_package_group}")
        sagemaker.create_model_package_group(
            ModelPackageGroupName=model_package_group,
            ModelPackageGroupDescription="Retail Forecast Models",
            Tags=[
                {'Key': 'Project', 'Value': 'RetailForecast'}
            ]
        )
    
    # L·∫•y th√¥ng tin training job
    training_job = sagemaker.describe_training_job(TrainingJobName=job_name)
    model_data_url = training_job['ModelArtifacts']['S3ModelArtifacts']
    image_uri = training_job['AlgorithmSpecification']['TrainingImage']
    
    # ƒêƒÉng k√Ω model package
    model_package_name = f"{model_package_group}-{int(time.time())}"
    
    create_model_package_input_dict = {
        'ModelPackageGroupName': model_package_group,
        'ModelPackageDescription': f"Model trained by job {job_name}",
        'InferenceSpecification': {
            'Containers': [
                {
                    'Image': image_uri,
                    'ModelDataUrl': model_data_url
                }
            ],
            'SupportedContentTypes': ['text/csv'],
            'SupportedResponseMIMETypes': ['text/csv']
        },
        'ModelApprovalStatus': approval_status,
        'CustomerMetadataProperties': {
            'TrainingJobName': job_name,
            'CreatedBy': 'GitHubActionsCI'
        },
        'Tags': [
            {'Key': 'Project', 'Value': 'RetailForecast'}
        ]
    }
    
    # Th√™m model metrics n·∫øu c√≥
    try:
        with open('evaluation_results.json', 'r') as f:
            eval_results = json.load(f)
        
        model_metrics = []
        for metric_name, value in eval_results.get('metrics', {}).items():
            model_metrics.append({
                'Name': metric_name,
                'Value': float(value)
            })
        
        if model_metrics:
            create_model_package_input_dict['ModelMetrics'] = {
                'ModelQuality': {
                    'Statistics': {
                        'ContentType': 'application/json',
                        'Values': model_metrics
                    }
                }
            }
    except Exception as e:
        print(f"Kh√¥ng th·ªÉ ƒë·ªçc k·∫øt qu·∫£ ƒë√°nh gi√°: {e}")
    
    response = sagemaker.create_model_package(**create_model_package_input_dict)
    
    print(f"Model package ƒë√£ ƒë∆∞·ª£c t·∫°o: {response['ModelPackageArn']}")
    
    # L·∫•y version t·ª´ ARN
    model_version = response['ModelPackageArn'].split('/')[-1]
    print(f"Model version: {model_version}")
    
    return model_version

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='Register model to SageMaker Model Registry')
    parser.add_argument('--job-name', type=str, required=True,
                        help='Name of the training job')
    parser.add_argument('--model-package-group', type=str, required=True,
                        help='Model package group name')
    parser.add_argument('--approval-status', type=str, default='PendingManualApproval',
                        choices=['Approved', 'Rejected', 'PendingManualApproval'],
                        help='Initial model approval status')
    
    args = parser.parse_args()
    
    model_version = register_model(
        args.job_name,
        args.model_package_group,
        args.approval_status
    )
    
    print(model_version)
```

### 4.4 Script tri·ªÉn khai Endpoint

```python
# aws/script/deploy_endpoint.py
import boto3
import argparse
import time
import uuid

def deploy_endpoint(model_package_arn, endpoint_name, instance_type, instance_count=1):
    """Deploy model to SageMaker endpoint"""
    
    sagemaker = boto3.client('sagemaker')
    timestamp = int(time.time())
    
    # T·∫°o model
    model_name = f"{endpoint_name}-model-{timestamp}"
    print(f"Creating model {model_name} from {model_package_arn}")
    
    model_response = sagemaker.create_model(
        ModelName=model_name,
        PrimaryContainer={
            'ModelPackageName': model_package_arn
        },
        ExecutionRoleArn=sagemaker.get_caller_identity()['RoleArn']
    )
    
    # T·∫°o endpoint config
    config_name = f"{endpoint_name}-config-{timestamp}"
    print(f"Creating endpoint config {config_name}")
    
    endpoint_config_response = sagemaker.create_endpoint_config(
        EndpointConfigName=config_name,
        ProductionVariants=[
            {
                'VariantName': 'default',
                'ModelName': model_name,
                'InstanceType': instance_type,
                'InitialInstanceCount': instance_count,
                'InitialVariantWeight': 1.0
            }
        ],
        Tags=[
            {'Key': 'Project', 'Value': 'RetailForecast'}
        ]
    )
    
    # Ki·ªÉm tra xem endpoint ƒë√£ t·ªìn t·∫°i ch∆∞a
    endpoint_exists = False
    try:
        response = sagemaker.describe_endpoint(EndpointName=endpoint_name)
        endpoint_exists = True
    except sagemaker.exceptions.ClientError:
        endpoint_exists = False
    
    # T·∫°o ho·∫∑c c·∫≠p nh·∫≠t endpoint
    if endpoint_exists:
        print(f"Updating endpoint {endpoint_name}")
        endpoint_response = sagemaker.update_endpoint(
            EndpointName=endpoint_name,
            EndpointConfigName=config_name
        )
    else:
        print(f"Creating endpoint {endpoint_name}")
        endpoint_response = sagemaker.create_endpoint(
            EndpointName=endpoint_name,
            EndpointConfigName=config_name,
            Tags=[
                {'Key': 'Project', 'Value': 'RetailForecast'}
            ]
        )
    
    # ƒê·ª£i endpoint s·∫µn s√†ng
    print(f"Waiting for endpoint {endpoint_name} to be in service...")
    waiter = sagemaker.get_waiter('endpoint_in_service')
    waiter.wait(EndpointName=endpoint_name)
    
    # L·∫•y th√¥ng tin endpoint
    endpoint_info = sagemaker.describe_endpoint(EndpointName=endpoint_name)
    print(f"Endpoint {endpoint_name} is ready (status: {endpoint_info['EndpointStatus']})")
    
    return {
        'endpoint_name': endpoint_name,
        'endpoint_arn': endpoint_info['EndpointArn'],
        'status': endpoint_info['EndpointStatus']
    }

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='Deploy model to SageMaker endpoint')
    parser.add_argument('--model-package-arn', type=str, required=True,
                        help='ARN of model package to deploy')
    parser.add_argument('--endpoint-name', type=str, required=True,
                        help='Name of the endpoint')
    parser.add_argument('--instance-type', type=str, default='ml.t2.medium',
                        help='Instance type for inference')
    parser.add_argument('--instance-count', type=int, default=1,
                        help='Number of instances')
    
    args = parser.parse_args()
    
    result = deploy_endpoint(
        args.model_package_arn,
        args.endpoint_name,
        args.instance_type,
        args.instance_count
    )
    
    print(f"Endpoint deployment complete: {result}")
```

### 4.5 Script ki·ªÉm tra hi·ªáu su·∫•t Endpoint

```python
# aws/script/autoscaling_endpoint.py
import boto3
import argparse
import json
import time
import datetime

def setup_autoscaling(endpoint_name, min_capacity=1, max_capacity=4, 
                      target_value=70.0, scale_in_cooldown=300, 
                      scale_out_cooldown=60):
    """Thi·∫øt l·∫≠p autoscaling cho SageMaker endpoint"""
    
    # L·∫•y variant name v√† ARN c·ªßa endpoint
    sm = boto3.client('sagemaker')
    endpoint = sm.describe_endpoint(EndpointName=endpoint_name)
    endpoint_arn = endpoint['EndpointArn']
    
    config_name = endpoint['EndpointConfigName']
    config = sm.describe_endpoint_config(EndpointConfigName=config_name)
    variant_name = config['ProductionVariants'][0]['VariantName']
    
    # Chu·∫©n b·ªã resource ID cho autoscaling
    resource_id = f"endpoint/{endpoint_name}/variant/{variant_name}"
    
    # Thi·∫øt l·∫≠p application autoscaling
    aas = boto3.client('application-autoscaling')
    
    # ƒêƒÉng k√Ω scalable target
    aas.register_scalable_target(
        ServiceNamespace='sagemaker',
        ResourceId=resource_id,
        ScalableDimension='sagemaker:variant:DesiredInstanceCount',
        MinCapacity=min_capacity,
        MaxCapacity=max_capacity
    )
    
    # T·∫°o scaling policy
    response = aas.put_scaling_policy(
        PolicyName=f"{endpoint_name}-scaling-policy",
        ServiceNamespace='sagemaker',
        ResourceId=resource_id,
        ScalableDimension='sagemaker:variant:DesiredInstanceCount',
        PolicyType='TargetTrackingScaling',
        TargetTrackingScalingPolicyConfiguration={
            'TargetValue': target_value,
            'PredefinedMetricSpecification': {
                'PredefinedMetricType': 'SageMakerVariantInvocationsPerInstance'
            },
            'ScaleInCooldown': scale_in_cooldown,
            'ScaleOutCooldown': scale_out_cooldown
        }
    )
    
    print(f"Autoscaling configured for endpoint {endpoint_name}")
    print(f"Min capacity: {min_capacity}, Max capacity: {max_capacity}")
    print(f"Target value: {target_value} invocations per instance")
    
    return {
        'endpoint_name': endpoint_name,
        'resource_id': resource_id,
        'scaling_policy_arn': response['PolicyARN']
    }

def load_test_endpoint(endpoint_name, test_data_path, duration=60, rate=10):
    """Test t·∫£i endpoint ƒë·ªÉ ki·ªÉm tra hi·ªáu su·∫•t v√† autoscaling"""
    
    sagemaker_runtime = boto3.client('sagemaker-runtime')
    
    # Load test data
    with open(test_data_path, 'r') as f:
        test_data = json.load(f)
    
    # N·∫øu test data l√† list, l·∫•y ph·∫ßn t·ª≠ ƒë·∫ßu ti√™n
    if isinstance(test_data, list):
        sample = test_data[0]
    else:
        sample = test_data
    
    # Chu·∫©n b·ªã test
    start_time = time.time()
    end_time = start_time + duration
    request_count = 0
    success_count = 0
    latencies = []
    
    print(f"Starting load test on endpoint {endpoint_name}")
    print(f"Duration: {duration} seconds, Rate: {rate} requests/second")
    
    # Th·ª±c hi·ªán load test
    while time.time() < end_time:
        batch_start = time.time()
        for _ in range(rate):
            if time.time() >= end_time:
                break
                
            try:
                # G·ª≠i request
                request_start = time.time()
                response = sagemaker_runtime.invoke_endpoint(
                    EndpointName=endpoint_name,
                    ContentType='application/json',
                    Body=json.dumps(sample)
                )
                
                # ƒêo latency
                latency = (time.time() - request_start) * 1000  # milliseconds
                latencies.append(latency)
                
                # ƒê·ªçc k·∫øt qu·∫£
                result = json.loads(response['Body'].read().decode())
                
                # C·∫≠p nh·∫≠t counter
                request_count += 1
                success_count += 1
                
                if request_count % 50 == 0:
                    print(f"Processed {request_count} requests...")
                
            except Exception as e:
                request_count += 1
                print(f"Error invoking endpoint: {e}")
        
        # ƒê·ª£i ƒë·∫øn ƒë·∫ßu gi√¢y ti·∫øp theo
        elapsed = time.time() - batch_start
        if elapsed < 1.0:
            time.sleep(1.0 - elapsed)
    
    # T√≠nh to√°n k·∫øt qu·∫£
    total_time = time.time() - start_time
    avg_rate = request_count / total_time
    success_rate = (success_count / request_count) * 100 if request_count > 0 else 0
    
    if latencies:
        avg_latency = sum(latencies) / len(latencies)
        min_latency = min(latencies)
        max_latency = max(latencies)
        p95_latency = sorted(latencies)[int(len(latencies) * 0.95)]
        p99_latency = sorted(latencies)[int(len(latencies) * 0.99)]
    else:
        avg_latency = min_latency = max_latency = p95_latency = p99_latency = 0
    
    # In k·∫øt qu·∫£
    results = {
        'endpoint': endpoint_name,
        'duration_seconds': total_time,
        'request_count': request_count,
        'success_count': success_count,
        'success_rate_percent': success_rate,
        'avg_requests_per_second': avg_rate,
        'latency_ms': {
            'avg': avg_latency,
            'min': min_latency,
            'max': max_latency,
            'p95': p95_latency,
            'p99': p99_latency
        }
    }
    
    print("\nLoad Test Results:")
    print(json.dumps(results, indent=2))
    
    with open('load_test_results.json', 'w') as f:
        json.dump(results, f, indent=2)
    
    return results

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='Setup autoscaling and test SageMaker endpoint')
    subparsers = parser.add_subparsers(dest='command')
    
    # Subparser cho autoscaling
    autoscaling_parser = subparsers.add_parser('autoscale')
    autoscaling_parser.add_argument('--endpoint-name', type=str, required=True)
    autoscaling_parser.add_argument('--min-capacity', type=int, default=1)
    autoscaling_parser.add_argument('--max-capacity', type=int, default=4)
    autoscaling_parser.add_argument('--target-value', type=float, default=70.0)
    
    # Subparser cho load testing
    loadtest_parser = subparsers.add_parser('loadtest')
    loadtest_parser.add_argument('--endpoint-name', type=str, required=True)
    loadtest_parser.add_argument('--test-data', type=str, required=True)
    loadtest_parser.add_argument('--duration', type=int, default=60)
    loadtest_parser.add_argument('--rate', type=int, default=10)
    
    args = parser.parse_args()
    
    if args.command == 'autoscale':
        setup_autoscaling(
            args.endpoint_name,
            args.min_capacity,
            args.max_capacity,
            args.target_value
        )
    elif args.command == 'loadtest':
        load_test_endpoint(
            args.endpoint_name,
            args.test_data,
            args.duration,
            args.rate
        )
    else:
        parser.print_help()
```

## 5. Gi√°m s√°t v√† Th√¥ng b√°o

### 5.1 Thi·∫øt l·∫≠p SNS Topic

```bash
# T·∫°o SNS topic cho th√¥ng b√°o deployment
aws sns create-topic --name retail-forecast-alerts

# ƒêƒÉng k√Ω email nh·∫≠n th√¥ng b√°o
aws sns subscribe \
  --topic-arn arn:aws:sns:us-east-1:<ACCOUNT_ID>:retail-forecast-alerts \
  --protocol email \
  --notification-endpoint team@example.com

# ƒêƒÉng k√Ω webhook Slack 
aws sns subscribe \
  --topic-arn arn:aws:sns:us-east-1:<ACCOUNT_ID>:retail-forecast-alerts \
  --protocol https \
  --notification-endpoint https://hooks.slack.com/services/XXXX/YYYY/ZZZZ
```

### 5.2 CloudWatch Dashboard cho CI/CD Pipeline

```bash
# T·∫°o CloudWatch dashboard cho pipeline
aws cloudwatch put-dashboard \
  --dashboard-name RetailForecast-CI-CD-Pipeline \
  --dashboard-body '{
    "widgets": [
        {
            "type": "metric",
            "x": 0,
            "y": 0,
            "width": 12,
            "height": 6,
            "properties": {
                "metrics": [
                    [ "AWS/SageMaker", "TrainingJobsCompleted", { "stat": "Sum", "period": 86400 } ],
                    [ "AWS/SageMaker", "TrainingJobsFailed", { "stat": "Sum", "period": 86400 } ]
                ],
                "view": "timeSeries",
                "stacked": false,
                "region": "us-east-1",
                "title": "SageMaker Training Jobs",
                "period": 300,
                "stat": "Sum"
            }
        },
        {
            "type": "metric",
            "x": 12,
            "y": 0,
            "width": 12,
            "height": 6,
            "properties": {
                "metrics": [
                    [ "AWS/ApplicationELB", "TargetResponseTime", "LoadBalancer", "app/retail-forecast/abcdef" ],
                    [ ".", "HTTPCode_Target_5XX_Count", ".", "." ],
                    [ ".", "HTTPCode_Target_4XX_Count", ".", "." ]
                ],
                "view": "timeSeries",
                "stacked": false,
                "region": "us-east-1",
                "title": "API Performance",
                "period": 300,
                "stat": "Average"
            }
        },
        {
            "type": "metric",
            "x": 0,
            "y": 6,
            "width": 12,
            "height": 6,
            "properties": {
                "metrics": [
                    [ "AWS/SageMaker", "Invocations", "EndpointName", "retail-forecast-prod", { "stat": "Sum", "period": 60 } ],
                    [ ".", "InvocationsPerInstance", ".", "." ]
                ],
                "view": "timeSeries",
                "stacked": false,
                "region": "us-east-1",
                "title": "SageMaker Endpoint Invocations",
                "period": 300
            }
        },
        {
            "type": "metric",
            "x": 12,
            "y": 6,
            "width": 12,
            "height": 6,
            "properties": {
                "metrics": [
                    [ "RetailForecast/Pipeline", "DeploymentFrequency" ],
                    [ ".", "FailureRate" ],
                    [ ".", "LeadTime" ]
                ],
                "view": "timeSeries",
                "stacked": false,
                "region": "us-east-1",
                "title": "CI/CD Metrics",
                "period": 86400,
                "stat": "Average"
            }
        }
    ]
}'
```

### 5.3 C·∫•u h√¨nh Pipeline Metrics

```python
# aws/script/pipeline_metrics.py
import boto3
import json
import argparse
from datetime import datetime, timedelta

def publish_pipeline_metrics(deploy_success=True, lead_time=None):
    """Publish CI/CD pipeline metrics to CloudWatch"""
    
    cloudwatch = boto3.client('cloudwatch')
    
    # Get date parts for daily metrics
    now = datetime.utcnow()
    today = now.strftime('%Y-%m-%d')
    
    # Increment deployment count
    cloudwatch.put_metric_data(
        Namespace='RetailForecast/Pipeline',
        MetricData=[
            {
                'MetricName': 'DeploymentCount',
                'Dimensions': [
                    {
                        'Name': 'Date',
                        'Value': today
                    }
                ],
                'Value': 1.0,
                'Unit': 'Count'
            }
        ]
    )
    
    # Add deployment success/failure
    cloudwatch.put_metric_data(
        Namespace='RetailForecast/Pipeline',
        MetricData=[
            {
                'MetricName': 'DeploymentSuccess',
                'Dimensions': [
                    {
                        'Name': 'Date',
                        'Value': today
                    }
                ],
                'Value': 1.0 if deploy_success else 0.0,
                'Unit': 'Count'
            }
        ]
    )
    
    # Add lead time if provided
    if lead_time is not None:
        cloudwatch.put_metric_data(
            Namespace='RetailForecast/Pipeline',
            MetricData=[
                {
                    'MetricName': 'LeadTimeMinutes',
                    'Dimensions': [
                        {
                            'Name': 'Date',
                            'Value': today
                        }
                    ],
                    'Value': lead_time,
                    'Unit': 'Minutes'
                }
            ]
        )
    
    print(f"Pipeline metrics published successfully")
    
def calculate_pipeline_kpis(days=30):
    """Calculate KPIs for CI/CD pipeline"""
    
    cloudwatch = boto3.client('cloudwatch')
    end_time = datetime.utcnow()
    start_time = end_time - timedelta(days=days)
    
    # Get deployment frequency
    deploy_count_response = cloudwatch.get_metric_statistics(
        Namespace='RetailForecast/Pipeline',
        MetricName='DeploymentCount',
        StartTime=start_time,
        EndTime=end_time,
        Period=86400 * days,  # entire period
        Statistics=['Sum']
    )
    
    # Get deployment success/failure
    deploy_success_response = cloudwatch.get_metric_statistics(
        Namespace='RetailForecast/Pipeline',
        MetricName='DeploymentSuccess',
        StartTime=start_time,
        EndTime=end_time,
        Period=86400 * days,  # entire period
        Statistics=['Sum']
    )
    
    # Get lead time
    lead_time_response = cloudwatch.get_metric_statistics(
        Namespace='RetailForecast/Pipeline',
        MetricName='LeadTimeMinutes',
        StartTime=start_time,
        EndTime=end_time,
        Period=86400 * days,  # entire period
        Statistics=['Average']
    )
    
    # Calculate KPIs
    deploy_count = deploy_count_response['Datapoints'][0]['Sum'] if deploy_count_response['Datapoints'] else 0
    deploy_success = deploy_success_response['Datapoints'][0]['Sum'] if deploy_success_response['Datapoints'] else 0
    
    # Calculate metrics
    deployments_per_day = deploy_count / days if days > 0 else 0
    failure_rate = (deploy_count - deploy_success) / deploy_count * 100 if deploy_count > 0 else 0
    avg_lead_time = lead_time_response['Datapoints'][0]['Average'] if lead_time_response['Datapoints'] else 0
    
    kpis = {
        'period_days': days,
        'total_deployments': deploy_count,
        'successful_deployments': deploy_success,
        'deployments_per_day': deployments_per_day,
        'failure_rate_percent': failure_rate,
        'avg_lead_time_minutes': avg_lead_time
    }
    
    print(json.dumps(kpis, indent=2))
    return kpis

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='Manage CI/CD pipeline metrics')
    subparsers = parser.add_subparsers(dest='command')
    
    # Subparser cho publish metrics
    publish_parser = subparsers.add_parser('publish')
    publish_parser.add_argument('--success', type=bool, default=True)
    publish_parser.add_argument('--lead-time', type=float, help='Lead time in minutes')
    
    # Subparser cho calculate KPIs
    kpi_parser = subparsers.add_parser('kpi')
    kpi_parser.add_argument('--days', type=int, default=30)
    
    args = parser.parse_args()
    
    if args.command == 'publish':
        publish_pipeline_metrics(args.success, args.lead_time)
    elif args.command == 'kpi':
        calculate_pipeline_kpis(args.days)
    else:
        parser.print_help()
```

## 6. Ki·ªÉm tra v√† X√°c th·ª±c

### 6.1 Checklist Ho√†n th√†nh

‚úÖ C√°c th√†nh ph·∫ßn ch√≠nh ƒë√£ tri·ªÉn khai:

- [x] **Pipeline GitHub Actions**: Workflow ƒë√£ ƒë∆∞·ª£c c·∫•u h√¨nh v·ªõi c√°c job ƒë·∫ßy ƒë·ªß
- [x] **IAM Role & Permissions**: Role cho GitHub Actions v·ªõi OIDC x√°c th·ª±c
- [x] **Automated Testing**: Unit tests, code quality check, data validation
- [x] **Model Training**: T·ª± ƒë·ªông trigger SageMaker training jobs 
- [x] **Model Evaluation & Registry**: ƒê√°nh gi√° model v√† ƒëƒÉng k√Ω v√†o Model Registry
- [x] **Docker Build**: T·ª± ƒë·ªông build v√† push image l√™n ECR
- [x] **EKS Deployment**: T·ª± ƒë·ªông c·∫≠p nh·∫≠t Kubernetes deployment
- [x] **Health Check**: Ki·ªÉm tra v√† x√°c nh·∫≠n API ho·∫°t ƒë·ªông sau deployment
- [x] **CloudWatch Alarms**: T·ª± ƒë·ªông c·∫•u h√¨nh alert cho deployment m·ªõi
- [x] **Notifications**: Th√¥ng b√°o k·∫øt qu·∫£ deployment qua SNS

### 6.2 Ki·ªÉm tra Pipeline

ƒê·ªÉ x√°c nh·∫≠n pipeline ho·∫°t ƒë·ªông ch√≠nh x√°c, th·ª±c hi·ªán c√°c b∆∞·ªõc sau:

1. **K√≠ch ho·∫°t pipeline th√¥ng qua commit m·ªõi**
   ```bash
   # Commit code m·ªõi
   echo "# Test CI/CD pipeline" >> README.md
   git add README.md
   git commit -m "Test: Trigger CI/CD pipeline"
   git push origin main
   
   # Ki·ªÉm tra GitHub Actions
   # Pipeline s·∫Ω t·ª± ƒë·ªông k√≠ch ho·∫°t trong v√≤ng 1 ph√∫t
   ```

2. **K√≠ch ho·∫°t pipeline th·ªß c√¥ng v·ªõi hu·∫•n luy·ªán model**
   ```bash
   # S·ª≠ d·ª•ng GitHub UI ƒë·ªÉ k√≠ch ho·∫°t workflow v·ªõi c√°c t√πy ch·ªçn:
   # - environment: prod
   # - retrain_model: true
   ```

3. **Ki·ªÉm tra ECR repository**
   ```bash
   # Ki·ªÉm tra image m·ªõi tr√™n ECR
   aws ecr describe-images \
     --repository-name retail-forecast \
     --query 'sort_by(imageDetails,& imagePushedAt)[-5:]'
   
   # Ki·ªÉm tra tag m·ªõi nh·∫•t
   aws ecr list-images \
     --repository-name retail-forecast \
     --filter "tagStatus=TAGGED" \
     --query 'imageIds[?contains(imageTag, `latest`)]'
   ```

4. **Ki·ªÉm tra SageMaker model v√† deployment**
   ```bash
   # Ki·ªÉm tra training job m·ªõi nh·∫•t
   aws sagemaker list-training-jobs \
     --sort-by CreationTime \
     --sort-order Descending \
     --max-items 5
   
   # Ki·ªÉm tra model package m·ªõi nh·∫•t
   aws sagemaker list-model-packages \
     --model-package-group-name retail-forecast-models \
     --sort-by CreationTime \
     --sort-order Descending \
     --max-items 5
   
   # Ki·ªÉm tra endpoint
   aws sagemaker describe-endpoint \
     --endpoint-name retail-forecast-prod
   ```

5. **Ki·ªÉm tra EKS deployment**
   ```bash
   # Ki·ªÉm tra deployment m·ªõi
   kubectl get deployments -n retail-forecast-prod
   
   # Ki·ªÉm tra pods
   kubectl get pods -n retail-forecast-prod
   
   # Ki·ªÉm tra image ƒë∆∞·ª£c s·ª≠ d·ª•ng
   kubectl describe deployment retail-forecast-api -n retail-forecast-prod | grep Image:
   
   # Ki·ªÉm tra history rollout
   kubectl rollout history deployment/retail-forecast-api -n retail-forecast-prod
   ```

6. **Test API endpoint**
   ```bash
   # L·∫•y endpoint URL
   ENDPOINT=$(kubectl get ingress -n retail-forecast-prod retail-forecast-ingress -o jsonpath='{.status.loadBalancer.ingress[0].hostname}')
   
   # Ki·ªÉm tra health endpoint
   curl -v http://$ENDPOINT/health
   
   # Test prediction endpoint
   curl -X POST \
     -H "Content-Type: application/json" \
     -d '{"store_id": 1, "item_id": 123, "date": "2023-12-01"}' \
     http://$ENDPOINT/predict
   ```

### 6.3 Monitoring Commands

```bash
# Ki·ªÉm tra CloudWatch metrics c·ªßa API
aws cloudwatch get-metric-statistics \
  --namespace "RetailForecast/prod" \
  --metric-name "Latency" \
  --dimensions Name=DeploymentId,Value=<latest-deployment-id> \
  --start-time $(date -u -d '1 hour ago' +%Y-%m-%dT%H:%M:%SZ) \
  --end-time $(date -u +%Y-%m-%dT%H:%M:%SZ) \
  --period 60 \
  --statistics Average

# Ki·ªÉm tra SageMaker endpoint metrics
aws cloudwatch get-metric-statistics \
  --namespace "AWS/SageMaker" \
  --metric-name "ModelLatency" \
  --dimensions Name=EndpointName,Value=retail-forecast-prod \
  --start-time $(date -u -d '1 hour ago' +%Y-%m-%dT%H:%M:%SZ) \
  --end-time $(date -u +%Y-%m-%dT%H:%M:%SZ) \
  --period 60 \
  --statistics Average

# Ki·ªÉm tra Pipeline metrics
aws cloudwatch get-metric-statistics \
  --namespace "RetailForecast/Pipeline" \
  --metric-name "DeploymentSuccess" \
  --start-time $(date -u -d '7 days ago' +%Y-%m-%dT%H:%M:%SZ) \
  --end-time $(date -u +%Y-%m-%dT%H:%M:%SZ) \
  --period 86400 \
  --statistics Sum

# Ki·ªÉm tra CloudWatch Logs cho pipeline
aws logs get-log-events \
  --log-group-name "/aws/sagemaker/TrainingJobs" \
  --log-stream-name <latest-training-job-name>
```

## 7. Best Practices & T·ªëi ∆∞u

### 7.1 Pipeline Optimization

- **Parallel Execution**: Ch·∫°y song song c√°c job kh√¥ng ph·ª• thu·ªôc nhau
- **Caching**: S·ª≠ d·ª•ng caching cho pip v√† Docker layers
- **Conditional Execution**: Ch·ªâ ch·∫°y c√°c job c·∫ßn thi·∫øt d·ª±a tr√™n lo·∫°i thay ƒë·ªïi
- **Matrix Builds**: Test tr√™n nhi·ªÅu phi√™n b·∫£n Python ho·∫∑c m√¥i tr∆∞·ªùng
- **Artifact Sharing**: S·ª≠ d·ª•ng artifacts ƒë·ªÉ chia s·∫ª d·ªØ li·ªáu gi·ªØa c√°c job

### 7.2 Security Best Practices

- **OIDC Integration**: S·ª≠ d·ª•ng federation thay v√¨ access keys
- **Least Privilege IAM**: Role v·ªõi quy·ªÅn h·∫°n t·ªëi thi·ªÉu c·∫ßn thi·∫øt
- **Secret Management**: S·ª≠ d·ª•ng GitHub Secrets ho·∫∑c AWS Secrets Manager
- **Image Scanning**: Ki·ªÉm tra l·ªói b·∫£o m·∫≠t trong container images
- **Network Security**: Gi·ªõi h·∫°n network access trong pipeline

### 7.3 Quality Gates

- **Code Quality Checks**: Linting, formatting, static analysis
- **Unit & Integration Tests**: Test t·ª± ƒë·ªông cho m·ªçi thay ƒë·ªïi code
- **Data Validation**: Ki·ªÉm tra t√≠nh to√†n v·∫πn v√† ch·∫•t l∆∞·ª£ng d·ªØ li·ªáu
- **Model Evaluation**: ƒê√°nh gi√° hi·ªáu su·∫•t model v·ªõi baseline metrics
- **Performance Testing**: Ki·ªÉm tra ƒë·ªô tr·ªÖ v√† kh·∫£ nƒÉng x·ª≠ l√Ω t·∫£i c·ªßa API

### 7.4 Observability

- **Pipeline Metrics**: Theo d√µi t·∫ßn su·∫•t deployment, th·ªùi gian th·ª±c hi·ªán
- **Model Monitoring**: Theo d√µi model drift v√† data drift
- **API Metrics**: Latency, error rate, throughput c·ªßa endpoints
- **Logging**: Log t·∫≠p trung v·ªõi c·∫•u tr√∫c th·ªëng nh·∫•t
- **Alerting**: C·∫£nh b√°o s·ªõm khi ph√°t hi·ªán v·∫•n ƒë·ªÅ

## T·ªïng k·∫øt

CI/CD pipeline ƒë√£ ƒë∆∞·ª£c thi·∫øt l·∫≠p ƒë·∫ßy ƒë·ªß s·ª≠ d·ª•ng GitHub Actions ƒë·ªÉ t·ª± ƒë·ªông h√≥a to√†n b·ªô quy tr√¨nh MLOps cho d·ª± √°n Retail Prediction:

1. **T√≠ch h·ª£p li√™n t·ª•c (CI)**: T·ª± ƒë·ªông test, lint, v√† build Docker image.
2. **Hu·∫•n luy·ªán t·ª± ƒë·ªông**: K√≠ch ho·∫°t SageMaker training jobs khi c·∫ßn thi·∫øt.
3. **Model Registry**: ƒê√°nh gi√° v√† ƒëƒÉng k√Ω model v·ªõi qu·∫£n l√Ω version.
4. **Deployment li√™n t·ª•c (CD)**: T·ª± ƒë·ªông tri·ªÉn khai l√™n EKS cluster.
5. **Gi√°m s√°t**: CloudWatch metrics v√† alerts cho to√†n b·ªô h·ªá th·ªëng.

Pipeline n√†y ƒë·∫£m b·∫£o quy tr√¨nh MLOps ƒë√°ng tin c·∫≠y, t·ª± ƒë·ªông, v√† c√≥ kh·∫£ nƒÉng m·ªü r·ªông, gi√∫p team c√≥ th·ªÉ t·∫≠p trung v√†o vi·ªác c·∫£i thi·ªán model v√† t√≠nh nƒÉng thay v√¨ c√¥ng vi·ªác v·∫≠n h√†nh th·ªß c√¥ng.

---

**Next Step**: [Task 14: Audit & Security](../14-security-audit/)

---

**Next Step**: [Task 15: DataOps - Data Upload & Versioning](../15-dataops-upload-versioning/)