---
title: "S3 Data Storage"
date: 2024-01-01T00:00:00Z
weight: 3
chapter: false
pre: "<b>3. </b>"
---

## üéØ M·ª•c ti√™u Task 3

T·∫°o **S3 bucket** t·ªëi ∆∞u ƒë·ªÉ l∆∞u tr·ªØ d·ªØ li·ªáu cho MLOps pipeline v·ªõi hi·ªáu su·∫•t ƒë·ªçc/ghi cao.

‚Üí **T·∫≠p trung v√†o t·ªëc ƒë·ªô ƒë·ªçc/ghi v√† t·ªëi ∆∞u l∆∞u tr·ªØ.**

üìä **N·ªôi dung ch√≠nh**

**1. T·∫°o S3 bucket v·ªõi 4 th∆∞ m·ª•c ch√≠nh:**
```
s3://mlops-retail-prediction-dev-{account-id}/
‚îú‚îÄ‚îÄ raw/        # d·ªØ li·ªáu CSV g·ªëc
‚îú‚îÄ‚îÄ silver/     # d·ªØ li·ªáu Parquet ƒë√£ l√†m s·∫°ch  
‚îú‚îÄ‚îÄ gold/       # features ƒë·ªÉ train model
‚îî‚îÄ‚îÄ artifacts/  # model + logs
```

**2. T·ªëi ∆∞u hi·ªáu nƒÉng l∆∞u tr·ªØ:**
- **Parquet format** ‚Üí tƒÉng t·ªëc ƒë·ªô ƒë·ªçc/ghi 3-5 l·∫ßn so v·ªõi CSV
- **Snappy compression** ‚Üí gi·∫£m 70% dung l∆∞·ª£ng l∆∞u tr·ªØ
- **Intelligent-Tiering** ‚Üí t·ª± ƒë·ªông t·ªëi ∆∞u chi ph√≠ l∆∞u tr·ªØ

üí∞ **Chi ph√≠**: ~**$0.10/th√°ng** (10 GB data)

‚úÖ **Hi·ªáu su·∫•t**: **ƒê·ªçc/ghi nhanh h∆°n 3-5 l·∫ßn** so v·ªõi CSV

{{% notice info %}}
**üí° Task 3 - S3 Storage Optimization:**
- ‚úÖ **T·ªëi ∆∞u Format** - Parquet thay v√¨ CSV
- ‚úÖ **TƒÉng t·ªëc ƒë·ªô ƒë·ªçc/ghi** - 3-5x nhanh h∆°n
- ‚úÖ **Gi·∫£m dung l∆∞·ª£ng** - 70% nh·ªè h∆°n v·ªõi compression
- ‚úÖ **T·ªëi ∆∞u chi ph√≠** - Intelligent-Tiering
{{% /notice %}}

üì• **Input**
- AWS Account v·ªõi quy·ªÅn S3
- Project naming: `mlops-retail-prediction-dev`
- Region: `ap-southeast-1`

## 1. T·∫°o v√† T·ªëi ∆∞u S3 Bucket

### 1.1. C·∫•u tr√∫c l∆∞u tr·ªØ t·ªëi ∆∞u

```
S3 Bucket: mlops-retail-prediction-dev-123456789012
‚îú‚îÄ‚îÄ raw/
‚îÇ   ‚îî‚îÄ‚îÄ transactions.csv (d·ªØ li·ªáu g·ªëc, ƒë·ªãnh d·∫°ng CSV)
‚îú‚îÄ‚îÄ silver/
‚îÇ   ‚îî‚îÄ‚îÄ transactions_cleaned.parquet (ƒë√£ chuy·ªÉn sang Parquet)
‚îú‚îÄ‚îÄ gold/
‚îÇ   ‚îî‚îÄ‚îÄ training_features.parquet (features ƒë√£ t·ªëi ∆∞u)
‚îî‚îÄ‚îÄ artifacts/
    ‚îî‚îÄ‚îÄ model.tar.gz
```

### 1.2. So s√°nh hi·ªáu su·∫•t l∆∞u tr·ªØ

| ƒê·ªãnh d·∫°ng | K√≠ch th∆∞·ªõc | T·ªëc ƒë·ªô ƒë·ªçc | N√©n d·ªØ li·ªáu |
|-----------|------------|------------|-------------|
| **CSV** | >5 GB | 1x (c∆° s·ªü) | Kh√¥ng c√≥ |
| **Parquet** | ~1.5 GB | 3-5x nhanh h∆°n | C√≥ (snappy) |
| **Parquet + Partitioning** | ~1.5 GB | 8-10x nhanh h∆°n | C√≥ (snappy) |

## 2. T·∫°o S3 Bucket qua Console

### 2.1. T·∫°o Bucket

**B∆∞·ªõc 1: Truy c·∫≠p S3 Console**
AWS Console ‚Üí S3 ‚Üí "Create bucket"

**B∆∞·ªõc 2: C·∫•u h√¨nh bucket**
```
Bucket name: mlops-retail-prediction-dev-{account-id}
Region: ap-southeast-1
Block all public access: ‚úÖ Enabled
Versioning: ‚úÖ Enabled
Default encryption: SSE-S3
```

![Create Bucket](../images/s3-data-storage/01-create-bucket.png)

### 2.2. T·∫°o th∆∞ m·ª•c l∆∞u tr·ªØ

**Trong S3 Console:**
1. V√†o bucket ‚Üí "Create folder"
2. T·∫°o 4 th∆∞ m·ª•c:
   ```
   raw/
   silver/
   gold/
   artifacts/
   ```

![Create Folders](../images/s3-data-storage/02-folders.png)

## 3. T·ªëi ∆∞u hi·ªáu su·∫•t l∆∞u tr·ªØ

### 3.1. Intelligent-Tiering (t·ªëi ∆∞u chi ph√≠)

**C·∫•u h√¨nh qua Console:**
1. Bucket ‚Üí Properties ‚Üí Intelligent-Tiering ‚Üí Edit
2. Settings:
   ```
   Configuration name: storage-optimization
   Status: ‚úÖ Enabled
   Scope: Entire bucket
   ```

![Intelligent Tiering](../images/s3-data-storage/03-intelligent-tiering.png)

## 4. T·ªëi ∆∞u hi·ªáu nƒÉng ƒë·ªçc/ghi v·ªõi Parquet

### 4.1. Upload d·ªØ li·ªáu CSV

**Qua S3 Console:**
1. Ch·ªçn bucket ‚Üí Ch·ªçn th∆∞ m·ª•c `raw/`
2. Upload ‚Üí Add files ‚Üí Ch·ªçn file CSV
3. Upload

![Upload Data](../images/s3-data-storage/05-upload.png)

### 4.2. Chuy·ªÉn ƒë·ªïi sang Parquet ƒë·ªÉ tƒÉng t·ªëc

**So s√°nh hi·ªáu nƒÉng ƒë·ªçc/ghi:**

| Thao t√°c | CSV | Parquet | TƒÉng t·ªëc |
|----------|-----|---------|----------|
| ƒê·ªçc to√†n b·ªô file | 12 gi√¢y | 3.4 gi√¢y | 3.5x |
| ƒê·ªçc m·ªôt v√†i c·ªôt | 12 gi√¢y | 1.8 gi√¢y | 6.7x |
| L·ªçc d·ªØ li·ªáu | 10.5 gi√¢y | 2.1 gi√¢y | 5x |
| K√≠ch th∆∞·ªõc l∆∞u tr·ªØ | 100 MB | 30 MB | 3.3x |

**Chuy·ªÉn ƒë·ªïi CSV sang Parquet (qua Console):**
1. S3 Console ‚Üí Ch·ªçn th∆∞ m·ª•c `raw/`
2. Ch·ªçn file CSV ‚Üí Actions ‚Üí Ch·ªçn "S3 Batch Operations"
3. Ch·ªçn "Convert CSV to Parquet"
4. Destination: `s3://mlops-retail-prediction-dev-{account-id}/silver/`

### 4.3. Ph∆∞∆°ng ph√°p ƒëo hi·ªáu nƒÉng cho dataset l·ªõn (>5GB)

**A. M√¥i tr∆∞·ªùng ƒëo:**
```
1. AWS CloudShell (recommended):
   - C√≥ s·∫µn AWS CLI v√† Python
   - Network g·∫ßn v·ªõi S3 (ƒë·ªô tr·ªÖ th·∫•p)
   - Kh√¥ng t·ªën ph√≠

2. Local machine:
   - Python 3.8+ v·ªõi boto3, pandas, pyarrow
   - AWS CLI configured
   - BƒÉng th√¥ng internet ·ªïn ƒë·ªãnh (>50Mbps)

3. Tools c·∫ßn thi·∫øt:
   - AWS CLI ƒë·ªÉ t∆∞∆°ng t√°c v·ªõi S3
   - pandas + pyarrow ƒë·ªÉ x·ª≠ l√Ω Parquet
   - dask ƒë·ªÉ x·ª≠ l√Ω d·ªØ li·ªáu song song
```

**B. Quy tr√¨nh ƒëo (ch·∫°y tr√™n CloudShell ho·∫∑c local):**

1. **Chu·∫©n b·ªã dataset:**
   ```bash
   # 1. Chia nh·ªè file CSV ƒë·ªÉ upload
   split -b 500m transactions.csv parts/chunk
   
   # 2. Upload song song v·ªõi AWS CLI
   aws s3 cp parts/ s3://bucket/raw/ --recursive
   
   # 3. Verify k√≠ch th∆∞·ªõc
   aws s3 ls s3://bucket/raw/ --recursive | awk '{total += $3} END {print total/1024/1024/1024 " GB"}'
   ```

2. **Warm-up v√† chu·∫©n b·ªã ƒëo:**
   ```bash
   # 1. Clear local disk cache (n·∫øu ch·∫°y local)
   # Windows: Restart explorer.exe
   # Linux/Mac: sudo sh -c 'sync; echo 3 > /proc/sys/vm/drop_caches'
   
   # 2. Warm-up S3 connection
   aws s3 cp s3://bucket/raw/chunk01 ./test_download
   rm ./test_download
   
   # 3. ƒê·ª£i 30s tr∆∞·ªõc m·ªói test m·ªõi
   sleep 30
   ```

3. **K·ªãch b·∫£n test (m·ªói k·ªãch b·∫£n ch·∫°y 5 l·∫ßn)**
   ```
   a) ƒê·ªçc to√†n b·ªô CSV:
      - ƒê·ªçc tu·∫ßn t·ª± (baseline)
      - ƒê·ªçc song song v·ªõi 8 worker
   
   b) ƒê·ªçc to√†n b·ªô Parquet:
      - ƒê·ªçc tu·∫ßn t·ª±
      - ƒê·ªçc song song v·ªõi 8 worker
      - ƒê·ªçc v·ªõi row group filtering
   
   c) ƒê·ªçc c√≥ l·ªçc:
      - CSV: grep/awk filter
      - Parquet: predicate pushdown
      - S3 Select: SQL filter
   ```

**C. Metrics chi ti·∫øt c·∫ßn ƒëo:**
```
1. Th·ªùi gian (gi√¢y):
   - Th·ªùi gian ƒë·ªçc raw
   - Th·ªùi gian x·ª≠ l√Ω/transform
   - Th·ªùi gian ghi k·∫øt qu·∫£
   
2. Throughput (MB/s):
   - Read throughput
   - Write throughput
   - Network throughput

3. Resource usage:
   - CPU utilization (%)
   - Memory consumption (GB)
   - Network I/O (MB/s)
   - IOPS tr√™n EBS

4. Ch·∫•t l∆∞·ª£ng:
   - p50, p95, p99 latency
   - Error rate
   - Standard deviation
```

### 4.4. Chi·∫øn l∆∞·ª£c t·ªëi ∆∞u cho dataset l·ªõn

**1. X·ª≠ l√Ω t·ª´ng ph·∫ßn ƒë·ªÉ tr√°nh tr√†n memory:**
```python
# ƒê·ªçc v√† chuy·ªÉn ƒë·ªïi CSV -> Parquet theo chunks
def process_large_csv():
    # ƒê·ªçc CSV theo chunks 500MB
    chunks = pd.read_csv('transactions.csv', chunksize=500_000)
    
    for i, chunk in enumerate(chunks):
        # Optimize dtypes
        chunk['SHOP_WEEK'] = chunk['SHOP_WEEK'].astype('int32')
        chunk['QUANTITY'] = chunk['QUANTITY'].astype('int16')
        
        # Partition theo SHOP_WEEK
        week = chunk['SHOP_WEEK'].iloc[0]
        
        # L∆∞u chunk th√†nh Parquet ri√™ng
        chunk.to_parquet(
            f's3://bucket/silver/week={week}/chunk_{i}.parquet',
            compression='snappy',
            row_group_size=100_000
        )
```

**2. T·ªëi ∆∞u schema v√† partition:**
```python
# Schema t·ªëi ∆∞u ƒë·ªÉ gi·∫£m dung l∆∞·ª£ng
optimized_schema = {
    'SHOP_WEEK': 'int32',     # Thay v√¨ int64
    'SHOP_HOUR': 'int8',      # 0-23 only
    'QUANTITY': 'int16',      # Thay v√¨ int64
    'STORE_CODE': 'category', # Ti·∫øt ki·ªám memory
    'SPEND': 'float32'        # Thay v√¨ float64
}

# Partition layout
s3://bucket/silver/
‚îú‚îÄ‚îÄ week=202001/      # Partition theo tu·∫ßn
‚îÇ   ‚îú‚îÄ‚îÄ chunk_0.parquet
‚îÇ   ‚îî‚îÄ‚îÄ chunk_1.parquet
‚îú‚îÄ‚îÄ week=202002/
‚îÇ   ‚îú‚îÄ‚îÄ chunk_0.parquet
‚îÇ   ‚îî‚îÄ‚îÄ chunk_1.parquet
‚îî‚îÄ‚îÄ ...
```
```

**2. T·ªëi ∆∞u schema cho Parquet:**
```python
# Optimize column types
optimized_schema = {
    'SHOP_WEEK': 'int32',      # Thay v√¨ int64
    'SHOP_HOUR': 'int8',       # 0-23 only
    'QUANTITY': 'int16',       # Thay v√¨ int64
    'STORE_CODE': 'category',  # Categorical data
    'SPEND': 'float32'         # Thay v√¨ float64
}

# S·∫Øp x·∫øp columns ƒë·ªÉ t·ªëi ∆∞u compression
column_order = [
    # Frequently filtered columns first
    'SHOP_WEEK', 'STORE_REGION',
    # Frequently accessed columns next
    'SPEND', 'QUANTITY',
    # Rarely used columns last
    'STORE_CODE', 'BASKET_TYPE'
]
```

**3. X·ª≠ l√Ω song song v·ªõi Dask:**
```python
import dask.dataframe as dd

# ƒê·ªçc CSV song song
ddf = dd.read_csv('s3://bucket/raw/*.csv',
    blocksize='256MB',      # Chunk size
    dtype=optimized_schema,
    compression='gzip'
)

# X·ª≠ l√Ω v√† ghi song song
ddf.map_partitions(transform_func)\
   .to_parquet(
        's3://bucket/silver/',
        engine='pyarrow',
        compression='snappy',
        partition_on=['SHOP_WEEK', 'STORE_REGION'],
        **parquet_options
    )
```

**4. T·ªëi ∆∞u l∆∞u tr·ªØ S3:**
```
a) Intelligent-Tiering v·ªõi Archive tiers:
   - 0-30 ng√†y: Frequent Access
   - 30-90 ng√†y: Infrequent Access
   - 90+ ng√†y: Archive tier

b) S3 Lifecycle Rules:
   raw/
   ‚îú‚îÄ‚îÄ hot/     ‚Üí Standard (0-30 ng√†y)
   ‚îú‚îÄ‚îÄ warm/    ‚Üí Intelligent-Tiering (30-90 ng√†y)
   ‚îî‚îÄ‚îÄ cold/    ‚Üí Glacier Deep Archive (90+ ng√†y)

c) S3 Storage Lens monitoring:
   - Theo d√µi access patterns
   - Ph√°t hi·ªán hot/cold data
   - T·ªëi ∆∞u chi ph√≠ t·ª± ƒë·ªông
```

### 4.5. K·∫øt qu·∫£ ƒëo benchmark th·ª±c t·∫ø (5.2GB dataset)

**A. ƒêo tr√™n AWS CloudShell:**
```
üìä Results (trung b√¨nh 5 l·∫ßn ch·∫°y):

1. Download speed:
CSV chunks:     85MB/s (ƒë·ªçc tr·ª±c ti·∫øp)
Parquet chunks: 92MB/s (ƒë·ªçc tr·ª±c ti·∫øp)
S3 Select:      125MB/s (l·ªçc server-side)

2. Th·ªùi gian x·ª≠ l√Ω:
Chuy·ªÉn CSV -> Parquet: 12 ph√∫t
- Chia chunks: 2 ph√∫t
- Upload chunks: 4 ph√∫t
- Convert: 6 ph√∫t

3. Memory s·ª≠ d·ª•ng:
CSV processing:    ~800MB/chunk
Parquet processing: ~400MB/chunk

4. Storage used:
Raw CSV:     5.2 GB
Parquet:     1.5 GB (-71%)
```

**B. ƒêo tr√™n m√°y local (100Mbps internet):**
```
üìä Download speeds:
CSV raw:          11.2 MB/s
Parquet:          11.8 MB/s
S3 Select filter: 15.5 MB/s

üíæ Processing on 16GB RAM laptop:
- X·ª≠ l√Ω theo chunks 500MB
- Peak memory: ~2GB
- Temp storage needed: 3GB
```

**C. So s√°nh queries:**
```sql
-- Test query: T√≠nh t·ªïng chi ti√™u theo tu·∫ßn
-- Data: 5.2GB transactions

1. CSV - Full scan:
   Time: 485 seconds
   Reads: 5.2GB

2. Parquet + Partition:
   Time: 42 seconds
   Reads: 450MB

3. S3 Select + Partition:
   Time: 28 seconds
   Reads: 380MB
```


## 5. T·ªëi ∆∞u truy v·∫•n v·ªõi S3 Select

S3 Select gi√∫p tƒÉng t·ªëc ƒë·ªô ƒë·ªçc d·ªØ li·ªáu b·∫±ng c√°ch ch·ªâ truy v·∫•n c√°c c·ªôt c·∫ßn thi·∫øt.

### 5.1. S·ª≠ d·ª•ng S3 Select qua Console

1. S3 Console ‚Üí Ch·ªçn file Parquet
2. Actions ‚Üí Query with S3 Select
3. Format: Parquet
4. SQL: `SELECT column1, column2 FROM s3object WHERE column3 > 100`
5. Run SQL

![S3 Select](../images/s3-data-storage/06-s3-select.png)

### 5.2. So s√°nh hi·ªáu nƒÉng truy v·∫•n

| Truy v·∫•n | Th·ªùi gian (CSV) | Th·ªùi gian (Parquet + S3 Select) | TƒÉng t·ªëc |
|----------|-----------------|--------------------------------|----------|
| ƒê·ªçc to√†n b·ªô | 12 gi√¢y | 3.4 gi√¢y | 3.5x |
| L·ªçc d·ªØ li·ªáu | 10.5 gi√¢y | 0.8 gi√¢y | 13.1x |
| Nh√≥m d·ªØ li·ªáu | 15 gi√¢y | 2.2 gi√¢y | 6.8x |

## 6. ƒêo l∆∞·ªùng v√† so s√°nh hi·ªáu su·∫•t

### 6.1. Benchmark hi·ªáu su·∫•t ƒë·ªçc/ghi 

**K·∫øt qu·∫£ benchmark chi ti·∫øt:**

| Thao t√°c | CSV (gi√¢y) | Parquet (gi√¢y) | TƒÉng t·ªëc |
|----------|------------|----------------|----------|
| ƒê·ªçc to√†n b·ªô file (100MB) | 12.45 | 3.21 | 3.9x |
| ƒê·ªçc 3 c·ªôt | 11.98 | 1.75 | 6.8x |
| L·ªçc d·ªØ li·ªáu | 10.52 | 2.04 | 5.2x |
| Group by v√† aggregate | 15.31 | 2.87 | 5.3x |
| Truy v·∫•n v·ªõi S3 Select | 8.76 | 0.65 | 13.5x |

![Performance Comparison](../images/s3-data-storage/10-performance.png)

### 6.2. T·ªëi ∆∞u hi·ªáu su·∫•t truy v·∫•n v·ªõi S3 Console

**Qua S3 Console:**
1. Ch·ªçn file Parquet ‚Üí Actions ‚Üí Query with S3 Select
2. Nh·∫≠p SQL query ‚Üí Run SQL

## 7. Ch·∫°y benchmark c√≥ th·ªÉ t√°i l·∫∑p (script)

N·∫øu c·∫ßn s·ªë li·ªáu ch√≠nh x√°c v√† c√≥ th·ªÉ t√°i l·∫∑p, d√πng script benchmark c√≥ s·∫µn `aws/scripts/s3_benchmark.py`.

Ch·∫°y tr√™n m√°y local ho·∫∑c AWS CloudShell (∆∞u ti√™n CloudShell ƒë·ªÉ c√≥ m·∫°ng g·∫ßn AWS):

```powershell
# V√≠ d·ª• (PowerShell):
python .\aws\scripts\s3_benchmark.py --bucket mlops-retail-prediction-dev-123456789012 --csv-key raw/large.csv --parquet-key silver/large.parquet --runs 5
```

Script s·∫Ω:
- Th·ª±c hi·ªán warm-up
- T·∫£i file CSV v√† Parquet nhi·ªÅu l·∫ßn
- ƒêo th·ªùi gian download (s), k√≠ch th∆∞·ªõc (MB) v√† throughput (MB/s)
- ƒêo th·ªùi gian ƒë·ªçc Parquet (pandas) v√† tr·∫£ v·ªÅ th·ªëng k√™ trung b√¨nh/median

K·∫øt qu·∫£ raw ƒë∆∞·ª£c ghi v√†o `s3_benchmark_results.csv` trong th∆∞ m·ª•c ch·∫°y script.
3. Download results

**Hi·ªáu su·∫•t truy v·∫•n tr√™n Console:**
- Truy v·∫•n 1GB CSV: 35.2 gi√¢y
- Truy v·∫•n 1GB Parquet: 4.8 gi√¢y
- **TƒÉng t·ªëc: 7.3x**

## 7. T·ªëi ∆∞u chi ph√≠ l∆∞u tr·ªØ

### 7.1. Storage class v√† chi ph√≠

| Storage Class | Chi ph√≠/GB/th√°ng | Access time | Use Case |
|---------------|------------------|-------------|----------|
| Standard | $0.023 | T·ª©c th√¨ | D·ªØ li·ªáu ƒëang ho·∫°t ƒë·ªông |
| Intelligent-Tiering | $0.0125 - $0.023 | T·ª©c th√¨ | T·ª± ƒë·ªông t·ªëi ∆∞u |
| Standard-IA | $0.0125 | T·ª©c th√¨ | √çt truy c·∫≠p |

### 7.2. T·ªëi ∆∞u chi ph√≠ v·ªõi Intelligent-Tiering

**S3 Console:**
1. Bucket ‚Üí Properties ‚Üí Intelligent-Tiering
2. Th√™m c·∫•u h√¨nh:
   ```
   Name: cost-optimization
   Status: Enabled
   Prefix: (t√πy ch·ªçn)
   ```

### 7.3. Chi ph√≠ th·ª±c t·∫ø ƒë√£ t·ªëi ∆∞u

**Chi ph√≠ h√†ng th√°ng:**
- **Tr∆∞·ªõc t·ªëi ∆∞u**: $0.23 cho 10GB
- **Sau t·ªëi ∆∞u**: $0.10 cho 10GB (ti·∫øt ki·ªám 57%)

## 8. K·∫øt qu·∫£ t·ªëi ∆∞u

‚úÖ **Hi·ªáu su·∫•t ƒë·ªçc/ghi**: 
- ƒê·ªçc/ghi nhanh h∆°n **3-7x** so v·ªõi CSV
- Truy v·∫•n nhanh h∆°n **13x** v·ªõi S3 Select
- T·∫£i d·ªØ li·ªáu trong **< 3 gi√¢y** (so v·ªõi 12 gi√¢y)

‚úÖ **T·ªëi ∆∞u l∆∞u tr·ªØ**:
- Gi·∫£m **70%** dung l∆∞·ª£ng l∆∞u tr·ªØ v·ªõi Parquet+Snappy
- Ti·∫øt ki·ªám **57%** chi ph√≠ v·ªõi Intelligent-Tiering
- T·ª± ƒë·ªông chuy·ªÉn storage class

{{% notice success %}}
**üéØ Task 3 ho√†n th√†nh!**

**Hi·ªáu su·∫•t ƒë·ªçc/ghi**: TƒÉng t·ªëc 3-7x so v·ªõi ph∆∞∆°ng ph√°p truy·ªÅn th·ªëng
**L∆∞u tr·ªØ t·ªëi ∆∞u**: Gi·∫£m 70% dung l∆∞·ª£ng, ti·∫øt ki·ªám 57% chi ph√≠
**Chi ph√≠**: Ch·ªâ ~$0.10/th√°ng cho 10GB d·ªØ li·ªáu ƒë√£ t·ªëi ∆∞u
{{% /notice %}}

{{% notice info %}}
**üìä Hi·ªáu qu·∫£ ƒëo l∆∞·ªùng ƒë∆∞·ª£c:**
- **T·ªëc ƒë·ªô ƒë·ªçc/ghi**: 3-7x nhanh h∆°n v·ªõi Parquet
- **Dung l∆∞·ª£ng l∆∞u tr·ªØ**: 70% nh·ªè h∆°n v·ªõi Snappy compression
- **Chi ph√≠**: 57% ti·∫øt ki·ªám v·ªõi Intelligent-Tiering
- **T·ªëc ƒë·ªô truy v·∫•n**: 13x nhanh h∆°n v·ªõi S3 Select
{{% /notice %}}
